{"meta":{"title":"Sandra_feng personal blog","subtitle":"一切都很美好","description":"","author":"Sandra_feng","url":"https://sandra-feng.github.io","root":"/"},"pages":[{"title":"关于本站","date":"2024-01-18T12:01:14.142Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"about/index.html","permalink":"https://sandra-feng.github.io/about/index.html","excerpt":"","text":"这是个人小站，完全出于兴趣爱好分享计算机相关知识，记录个人生活学习。所有技术类文章除特别说明外均为本站原创，所提供的软件为院长原创或由网络整理而来。 本院始终坚信： 分享是种美德，好人一生平安！ 如果您也一样热爱互联网，喜欢捣鼓软件，欢迎评论留言或与作者联系。 QQ：joker 1729745257"},{"title":"archives","date":"2020-07-18T05:10:34.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"archives/index.html","permalink":"https://sandra-feng.github.io/archives/index.html","excerpt":"","text":""},{"title":"分类","date":"2020-08-30T03:41:17.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"categories/index.html","permalink":"https://sandra-feng.github.io/categories/index.html","excerpt":"","text":""},{"title":"contact","date":"2020-08-30T14:45:09.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"contact/index.html","permalink":"https://sandra-feng.github.io/contact/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2020-08-30T14:40:51.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"friends/index.html","permalink":"https://sandra-feng.github.io/friends/index.html","excerpt":"","text":""},{"title":"","date":"2016-12-31T10:09:56.000Z","updated":"2016-12-31T10:09:56.000Z","comments":false,"path":"history/index.html","permalink":"https://sandra-feng.github.io/history/index.html","excerpt":"","text":""},{"title":"友链","date":"2022-05-28T03:09:14.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"link/index.html","permalink":"https://sandra-feng.github.io/link/index.html","excerpt":"","text":""},{"title":"","date":"2017-02-24T09:37:05.000Z","updated":"2017-02-24T09:37:05.000Z","comments":true,"path":"tags/index.html","permalink":"https://sandra-feng.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Hello World","slug":"hello-world","date":"2024-01-18T12:01:14.142Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"hello-world.html","link":"","permalink":"https://sandra-feng.github.io/hello-world.html","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"科研工具","slug":"科研工具推荐","date":"2024-01-18T06:04:21.000Z","updated":"2024-01-18T06:04:21.000Z","comments":true,"path":"科研工具推荐.html","link":"","permalink":"https://sandra-feng.github.io/%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7%E6%8E%A8%E8%8D%90.html","excerpt":"","text":"科研工具大文件快速传输工具：奶牛快传 论文代码寻找：Papers with code 论文写作：网页版latex 公式代码转换：matcha可以转换为latex代码 期刊会议查询：https://ccf.atom.im/ 公益学术平台：https://pubscholar.cn/","categories":[{"name":"学编程","slug":"学编程","permalink":"https://sandra-feng.github.io/categories/%E5%AD%A6%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"混技能","slug":"混技能","permalink":"https://sandra-feng.github.io/tags/%E6%B7%B7%E6%8A%80%E8%83%BD/"}]},{"title":"多模态大模型","slug":"多模态大模型","date":"2023-11-12T06:04:21.000Z","updated":"2023-11-12T06:04:21.000Z","comments":true,"path":"多模态大模型.html","link":"","permalink":"https://sandra-feng.github.io/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B.html","excerpt":"","text":"多模态大模型CLIP: 连接文本和图像的桥梁CLIP的英文全称是Contrastive Language-Image Pre-training，即一种基于对比文本-图像对的预训练方法或者模型。CLIP是一种基于对比学习的多模态模型。CLIP的训练数据是文本-图像对：一张图像和它对应的文本描述，这里希望通过对比学习，模型能够学习到文本-图像对的匹配关系。有两个 encoder，一个对应图片，一个对应文本，图像和文本经过各自的 encoder 后，通过简单的点乘来代表不同模态的交互（相似性）。 1234567891011121314151617181920# image_encoder - ResNet or Vision Transformer# text_encoder - CBOW or Text Transformer# I[n, h, w, c] - minibatch of aligned images# T[n, l] - minibatch of aligned texts# W_i[d_i, d_e] - learned proj of image to embed# W_t[d_t, d_e] - learned proj of text to embed# t - learned temperature parameter# extract feature representations of each modalityI_f = image_encoder(I) #[n, d_i]T_f = text_encoder(T) #[n, d_t]# joint multimodal embedding [n, d_e]I_e = l2_normalize(np.dot(I_f, W_i), axis=1)T_e = l2_normalize(np.dot(T_f, W_t), axis=1)# scaled pairwise cosine similarities [n, n]logits = np.dot(I_e, T_e.T) * np.exp(t)# symmetric loss functionlabels = np.arange(n)loss_i = cross_entropy_loss(logits, labels, axis=0)loss_t = cross_entropy_loss(logits, labels, axis=1)loss = (loss_i + loss_t)/2 ALBEF：先对齐后融合ALBEF 可以说是结合了单流模型与双流模型，其模型由图像编码器（image encoder）、文本编码器（text encoder）和 多模态编码器（multimodal encoder）组成，其中图像编码器采用的是VIT模型，而文本编码器、多模态编码器采用是一个12层的bert模型，其前6层作为文本编码器，后6层作为多模态编码器。图像编码器和文本编码器分别提到的特征可以用对比损失函数进行特征对齐，而后将图像特征和文本特征统一输入到多模态编码器，可以利用常用的MLM和ITM来进行预训练。在 ALBEF 之前，多模态方法通常使用 transformer 的多模态编码器来同时编码视觉和文本特征，由于目标检测器是提前训练好的，因此视觉和文本特征并不是对齐的。图像和文本特征可能距离很远，这使得多模态编码器难以学习到它们之间的交互。为了解决这个问题，ALBEF 通过一个对比损失（也就是 CLIP 中的 ITC 损失）在进行多模态交互之前对齐图像和文本数据。 ITC（Image-Text Contrastive）损失函数的核心思想是，模型应该学会将图像和对应的描述性文本映射到特征空间中的相近点，同时将不相匹配的图像和文本映射到远离的点。这通常通过一个对比损失函数来实现，最常用的是InfoNCE损失，它是由一个softmax函数定义的。 在CLIP模型中，一个batch中的每个图像都会和每个文本描述计算一个相似度分数。这些分数被组织成一个相似度矩阵。然后，对于每个图像，其与正确文本描述的相似度分数（即正样本）应该比与任何错误描述的分数（即负样本）都要高。 以下是计算过程： ALBEF预训练任务分为图文对比（Image-Text Contrastive Learning）、.掩码建模（Masked Language Modeling）、.图文匹配（Image-Text Matching）三个任务。Masked Language Modeling 同时利用图像和上下文文本来预测mask词 下面红色框其实就类似于 CLIP，双塔各自编码图像和文本，然后取 CLS 进行对比学习； 上面蓝色框就是为了加强不同模态交互用的编码器（前面提到过 CLIP 内积的方式太简单了，这里就是加强多模态融合以适配更难的任务）； 图像编码器 12 层，文本编码器 6 层，多模态编码器 6 层；其实右侧是将一个 12 层的文本编码器拆成了两部分，这是因为一些研究工作发现在多模态中需要更强的图像编码器，进行这样的拆分一定程度上保证了强图像 encoder 和弱文本 encoder，且保证了模型参数不过多的情况下融合图像和文本的信息。 ALBEF总的训练目标就是这三个损失：L_{ITC}+ L_{MLM}+L_{ITM} 动量蒸馏的目的是通过在训练过程中引入一个额外的模型，即动量编码器，来增强模型学习到的特征对齐的稳定性和一致性。这个动量编码器是一个慢慢更新的模型版本，它提供了一个更加平滑和稳定的特征表示目标，帮助主模型更好地学习对齐特征。 动量 (Momentum) 在这个上下文中，“动量”通常指的是模型参数更新的一种方法，该方法在更新参数时结合了过去的参数。具体来说，在动量方法中，模型的参数更新不仅取决于当前梯度，还取决于之前梯度的累积。这种方法可以看作是物理中动量的概念的应用，有助于模型参数在优化过程中更平滑地移动，从而减少震荡并加速收敛。 在ALBEF的架构中，动量更新通常应用于动量模型（Momentum Model），该模型是主模型的一个影子副本。动量模型的参数是通过应用动量系数（�β）对主模型参数的指数加权移动平均得到的。这意味着动量模型的更新慢于主模型，因此可以提供更稳定的表示供主模型在训练过程中学习。 在提供的图中，动量更新部分对应于图中右侧的“momentum update”箭头，它指向动量模型（Momentum Model）。 蒸馏 (Distillation) “蒸馏”在机器学习中通常指知识蒸馏（Knowledge Distillation），这是一种模型压缩技术，其中一个较大或较复杂的模型（称为教师模型）的知识被转移到一个较小或较简单的模型（称为学生模型）。在ALBEF模型的上下文中，蒸馏被用来将动量模型的平滑和稳定的表示传递给主模型，以提高训练的质量和稳定性。 在蒸馏过程中，主模型被训练以模仿动量模型的输出。这样做的原因是动量模型的输出更加平滑和一致，因此它可以作为一个好的目标，引导主模型在特征空间中正确对齐图像和文本。 ALBEF的使用： 12345678910111213141516171819202122# 加载预训练的ALBEF模型model = load_pretrained_albef_model()# 对图像进行预处理preprocessed_images = preprocess_images(raw_images)# 对文本进行预处理preprocessed_texts = preprocess_texts(raw_texts)# 获取图像和文本的特征表示with torch.no_grad(): image_features = model.image_encoder(preprocessed_images) text_features = model.text_encoder(preprocessed_texts)# 对于需要融合特征的任务，可以进一步处理multimodal_features = model.multimodal_encoder(image_features, text_features)# 使用提取的特征进行下游任务for task_data in downstream_task_dataset: features = extract_features(task_data, model) # 根据任务提取相应的特征 predictions = downstream_model(features) # 可能需要一个额外的下游模型 # 进行任务相关的处理，例如计算损失，进行反向传播等 BLIP：统一理解和生成的自举多模态模型 虽然有三个模型，但是大部分参数都是共享的。 左一为 Image Encoder（图像编码器）：该组件使用 Vision Transformer（ViT）对图像进行编码，将全局图像特征表示为一个额外的[CLS]标记。 左二为 Text Encoder，采用了 BERT 的结构，提取文本特征用于与视觉特征计算 ITC loss。Text Encoder 不与视觉特征计算交叉注意力。 左三为 Image-grounded Text Encoder（基于图像的文本编码器），该组件通过在每个 Transformer 块的自注意力（Self-Attention）层和前馈神经网络（Feed Forward Network）之间插入一个交叉注意力（Cross-Attention）层，将视觉信息注入到文本编码中，提取文本特征用于计算 ITM 损失。 左四为 Imagegrounded Text Decoder（基于图像的文本解码器），用于进行 LM 语言建模训练（这里不再是用 MLM 了），生成与图像相关的文本描述。 三个文本编解码器分别为在文本前添加 [CLS]、[Encode]、[Decode] token 与 ALBEF 一样，同样采用动量模型为 ITC 生成伪标签；使用 ITC 为 ITM 进行难负例挖掘。 BLIP 的训练流程 字幕器 Captioner：给一张网络图片，生成字幕。它是一个视觉文本解码器，在 COCO 数据集上使用 LM 目标函数微调。给定网络图片 ，Captioner 生成字幕 。 过滤器 Filter：过滤掉噪声图文对。它是一个视觉文本编码器，看文本是否与图像匹配，在 COCO 数据集上使用 ITC 和 ITM 目标函数微调。Filter 删除原始 Web 文本 和合成文本 中的嘈杂文本，如果 ITM 头将其预测为与图像不匹配，则认为文本有噪声。 最后，将过滤后的图像-文本对与人工注释对相结合，形成一个新的数据集，作者用它来预训练一个新的模型。以此来过滤掉噪声信息。 CoCa（Contrastive Captioners）视觉-语言联合表示 模型架构： CoCa模型通常包括一个图像编码器和一个文本编码器，这两个编码器可以是基于Transformer的架构，如Vision Transformer (ViT) 用于图像编码，BERT或GPT用于文本编码。 多模态学习： 在预训练阶段，CoCa模型使用大量的图像-文本配对数据来学习这两种模态之间的相互关系。通过对比学习和自回归语言建模相结合的方式，模型被训练来理解图像内容和相应的文本描述。 对比学习： CoCa模型使用对比学习来确保图像和相应的文本在特征空间中被拉近，而不匹配的图像和文本被推远。这通常涉及到计算正负样本对之间的相似性，并通过对比损失函数来优化。 自回归语言建模： 为了更好地理解和生成文本，CoCa模型还包括一个自回归语言模型组件，它预测文本序列中的下一个词，以学习文本的内部结构。 使用coca获取多模态表示： 123456789101112131415161718192021222324from transformers import CoCaModel, CoCaTokenizermodel = CoCaModel.from_pretrained(&#x27;coca-base&#x27;)tokenizer = CoCaTokenizer.from_pretrained(&#x27;coca-base&#x27;)# 假设你有一个图像处理函数和文本处理函数processed_images = image_preprocessing(your_images)processed_texts = tokenizer(your_texts, padding=True, truncation=True, return_tensors=&quot;pt&quot;)# 获取多模态表示outputs = model(input_ids=processed_texts[&#x27;input_ids&#x27;], pixel_values=processed_images)multimodal_representation = outputs.pooler_output# 使用多模态表示进行下游任务downstream_model = SomeDownstreamTaskModel()predictions = downstream_model(multimodal_representation)# 微调模型outputs = model(input_ids=processed_texts[&#x27;input_ids&#x27;], pixel_values=processed_images, labels=your_labels)loss = outputs.lossloss.backward()optimizer.step() 它是使用对比损失和文本生成损失进行训练，也就是使用了 ITC 和 LM loss；这里没有使用 ITM loss，减少了模型参数每次迭代所需前向传播的次数，从而降低了训练时间。","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"https://sandra-feng.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"}],"tags":[{"name":"学习记录","slug":"学习记录","permalink":"https://sandra-feng.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}]},{"title":"bert各层输出","slug":"bert各层输出","date":"2023-10-30T06:04:21.000Z","updated":"2023-10-30T06:04:21.000Z","comments":true,"path":"bert各层输出.html","link":"","permalink":"https://sandra-feng.github.io/bert%E5%90%84%E5%B1%82%E8%BE%93%E5%87%BA.html","excerpt":"","text":"bert各层输出在最新的transformers接口中，我们获取bert的各个层输出，需要这样： 123456789101112131415from transformers import BertTokenizer, BertModelimport torchtokenizer = BertTokenizer.from_pretrained(&#x27;bert-base-uncased&#x27;)model = BertModel.from_pretrained(&#x27;bert-base-uncased&#x27;)inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)outputs = model(**inputs)last_hidden_states = outputs.last_hidden_state last_hidden_state = outputs.last_hidden_statepooler_output = outputs.pooler_outputhidden_states = outputs.hidden_statesattentions = outputs.attentions last_hidden_state：shape是(batch_size, sequence_length, hidden_size)，hidden_size&#x3D;768,它是模型最后一层输出的隐藏状态。（通常用于命名实体识别）pooler_output：shape是(batch_size, hidden_size)，这是序列的第一个token(classification token)的最后一层的隐藏状态，它是由线性层和Tanh激活函数进一步处理的。（通常用于句子分类，至于是使用这个表示，还是使用整个输入序列的隐藏状态序列的平均化或池化，视情况而定）hidden_states：这是输出的一个可选项，如果输出，需要指定config.output_hidden_states&#x3D;True,它也是一个元组，它的第一个元素是embedding，其余元素是各层的输出，每个元素的形状是(batch_size, sequence_length, hidden_size)attentions：这也是输出的一个可选项，如果输出，需要指定config.output_attentions&#x3D;True,它也是一个元组，它的元素是每一层的注意力权重，用于计算self-attention heads的加权平均值。 我们知道bert由12个transformer组成，需要用到这12个transformer块的时候就用hidden_states。使用前的实例化需要配置。 123config = BertConfig.from_pretrained( &#x27;bert-base-uncased&#x27;, output_hidden_states=True, output_attentions=True)bert = BertModel.from_pretrained(&#x27;bert-base-uncased&#x27;,config = config)","categories":[{"name":"代码修改笔记","slug":"代码修改笔记","permalink":"https://sandra-feng.github.io/categories/%E4%BB%A3%E7%A0%81%E4%BF%AE%E6%94%B9%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"学习记录","slug":"学习记录","permalink":"https://sandra-feng.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}]},{"title":"安装CLIP库及数据集下载网站","slug":"安装CLIP","date":"2023-10-30T06:04:21.000Z","updated":"2023-10-30T06:04:21.000Z","comments":true,"path":"安装CLIP.html","link":"","permalink":"https://sandra-feng.github.io/%E5%AE%89%E8%A3%85CLIP.html","excerpt":"","text":"安装CLIP及数据集下载网站安装pip install clip失败 用pip install openai-clip 数据集下载网站Google Dataset Search：https://datasetsearch.research.google.com/ kaggle: https://www.kaggle.com/datasets 数据堂：https://www.datatang.com/data 竞赛数据集：https://www.datafountain.cn/ 数据之家：http://www.escience.cn/people/data/index.html 数据集市场：https://www.datamarket.com/ 数据集搜索引擎：https://www.data.gov/ 数据集大全：https://www.dataquest.io/blog/free-datasets-for-projects/ 数据集网：https://www.dataset.net.cn/ openlab:https://opendatalab.com/","categories":[{"name":"学编程","slug":"学编程","permalink":"https://sandra-feng.github.io/categories/%E5%AD%A6%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"学习记录","slug":"学习记录","permalink":"https://sandra-feng.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}]},{"title":"wine的使用方法","slug":"ubuntu下安装windows软件的wine使用","date":"2023-10-28T06:04:21.000Z","updated":"2023-10-28T06:04:21.000Z","comments":true,"path":"ubuntu下安装windows软件的wine使用.html","link":"","permalink":"https://sandra-feng.github.io/ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85windows%E8%BD%AF%E4%BB%B6%E7%9A%84wine%E4%BD%BF%E7%94%A8.html","excerpt":"","text":"wine的使用方法1.下载 先安装wine，可以使windows上的程序在linux上运行。 1234sudo dpkg --add-architecture i386sudo apt updatesudo apt install wine64 wine32wine --version 在官网下载绿色版的mobaxterm，解压进入解压后的文件夹，打开终端利用wine安装 1wine msiexec /i /installer.msi wine安装的软件需要到home目录下，显示隐藏文件，进入wine文件夹 [ 进入device_c进入program Files(x86)就可以看到下载的windows软件了 想要打开这个软件需要进入该文件夹，找到exe文件，在终端打开输入 wine xxx.exe 服务器开启SSH服务，客户机创建SSH会话。","categories":[{"name":"学编程","slug":"学编程","permalink":"https://sandra-feng.github.io/categories/%E5%AD%A6%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"混技能","slug":"混技能","permalink":"https://sandra-feng.github.io/tags/%E6%B7%B7%E6%8A%80%E8%83%BD/"}]},{"title":"安装pytorch环境(GPU版本)","slug":"安装pytorch环境-GPU版本","date":"2023-10-20T06:04:21.000Z","updated":"2023-10-20T06:04:21.000Z","comments":true,"path":"安装pytorch环境-GPU版本.html","link":"","permalink":"https://sandra-feng.github.io/%E5%AE%89%E8%A3%85pytorch%E7%8E%AF%E5%A2%83-GPU%E7%89%88%E6%9C%AC.html","excerpt":"","text":"安装pytorch环境(GPU版本)离线安装比较简单 根据代码的需求进行配置环境。一般来说python&#x3D;3.7版本的就已经可以了。 第一步：查看电脑对应显卡cuda版本 win+r打开运行，输入cmd打开命令行，输入nvidia-smi，查看cuda版本。 说明我可以下载cuda12.0以下版本，cuda向下兼容。 CUDA版本与PyTorch版本之间的兼容性通常不是一一对应的，而是根据PyTorch的发布周期和支持策略来确定的。一般来说，PyTorch的不同版本会添加对不同CUDA版本的支持，但并不是每个PyTorch版本都与每个CUDA版本完全兼容。一般情况下，较新的CUDA版本通常可以兼容较旧的PyTorch版本。 例如，如果你安装了最新版本的CUDA，并且你选择一个较旧的PyTorch版本，通常情况下是可以正常工作的，因为较新的CUDA通常包含对较旧CUDA API的支持。然而，要注意的是，较旧的PyTorch版本可能不会利用较新CUDA版本的新特性和性能优化。 但是，反过来并不总是成立。较新的PyTorch版本可能会依赖于新的CUDA特性或API，因此可能不兼容较旧的CUDA版本。 总之，PyTorch和CUDA之间的兼容性通常是向后兼容的。 CUDA版本 PyTorch版本 11.3 1.7.1 11.4 1.8.0 11.4 1.9.0 11.4 1.9.1 11.4 1.10.0 第二步：在下面网站中找到对应的torch和torchvision，下载whl文件 下载网站：https://download.pytorch.org/whl/torch_stable.html 在这里可以查看torch,torchversion的对应关系mirrors &#x2F; pytorch &#x2F; vision · GitCode 或者在这里：以前的 PyTorch 版本 |PyTorch 这里出来的最新的版本对应关系是cuda11.8的，我们12.0的也完全够用。 由上图，我们来下载torch&#x3D;2.0的，torchvision&#x3D;0.15.0，torchaudio&#x3D;&#x3D;2.0.0，我选择了python&#x3D;3.9的，因为怕3.10以上的出问题。 cu113表示cuda版本是11.3cp37 表示python版本3.7win-amd64 表示windows64位 下载完安装包后进行第三步 第三步：打开anaconda prompt执行以下命令 1conda create -n torch2python39 python=3.9 torch2python39是自己命名的环境。 12345conda activate torch2python39cd D:\\Anaconda\\安装包 #安装包文件夹pip install torchvision-0.15.0+cu118-cp39-cp39-win_amd64.whl #先安装了torchvisionpip install torchaudio-2.0.0+cu118-cp39-cp39-win_amd64.whlpip install torchaudio-2.0.0+cu118-cp39-cp39-win_amd64.whl 最后激活python环境，输入import torch，print(torch.cuda.is_available())得到True说明torch环境安装成功。","categories":[{"name":"学编程","slug":"学编程","permalink":"https://sandra-feng.github.io/categories/%E5%AD%A6%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"混技能","slug":"混技能","permalink":"https://sandra-feng.github.io/tags/%E6%B7%B7%E6%8A%80%E8%83%BD/"}]},{"title":"TRIMOON:基于两轮不一致性的假新闻检测多模态融合网络","slug":"TRIMOON基于两轮不一致性的假新闻检测多模态融合网络","date":"2023-08-30T06:04:21.000Z","updated":"2023-08-30T06:04:21.000Z","comments":true,"path":"TRIMOON基于两轮不一致性的假新闻检测多模态融合网络.html","link":"","permalink":"https://sandra-feng.github.io/TRIMOON%E5%9F%BA%E4%BA%8E%E4%B8%A4%E8%BD%AE%E4%B8%8D%E4%B8%80%E8%87%B4%E6%80%A7%E7%9A%84%E5%81%87%E6%96%B0%E9%97%BB%E6%A3%80%E6%B5%8B%E5%A4%9A%E6%A8%A1%E6%80%81%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C.html","excerpt":"","text":"TRIMOON:基于两轮不一致性的假新闻检测多模态融合网络然而，有时新闻文章的文本和图像之间存在不一致。因此，为了判断图像和文本的一致性，Xue等[16]提出了一种多模态一致性神经网络(multi-modal consistency neural network, MCNN)，其中包含相似度测量模块，使用余弦距离来测量图像和文本之间的一致性。本实验考虑了多模态数据的一致性，大大增强了对虚假新闻+图像不匹配情况的检测。在基于多模态的假新闻检测方面，该模型比其他基线模型更有效。 综上所述，虽然现有的研究主要集中在图文多模态融合和图文一致性检测两种方法上，也取得了较好的表现，但目前的方法仍然面临以下挑战:(1)多模态特征融合和图文一致性判别是独立进行的，导致最终的融合特征中存在噪声。(2)由于文本是新闻文章的主体，在现有的模式中，文本情态中的信息在表达信息中的主导作用还没有充分体现出来。因此，如何充分考虑图像和文本特征的融合，突出文本在新闻中的主导地位，同时兼顾图像和文本的一致性问题，还有待探索。 为了解决上述挑战，我们提出了一种考虑图像-文本不一致性的多模态融合模型，称为基于两轮不一致性的多模态融合网络(TRIMOON)。 首先，我们在一般的图像-文本共注意融合模块之前构建了语义一致性评分模块，目的是基于一致性程度来控制融合强度。其次，我们进行了文本形态的融合和图像-文本融合表示，以加强文本信息的主导地位。最后，在融合表示的基础上，利用双向长短期记忆(BiLSTM)对文档表示进行编码，并通过全连通层输出分类标签。 本文的主要贡献如下: 本文提出了一种基于图像-文本一致性的多模态融合模块用于假新闻检测（本人想法：还可以融合其他模态比如情感特征、上下文特征）抑制图像和文本不一致时融合表示产生的噪声。 我们为图像-文本信息融合的表征提供了一种二次融合机制，从而加强了文本模式在新闻媒体中的主导作用。 本文的后续章节组织如下。 第2节提供了相关工作的分析，并总结了我们提出的方法与现有方法之间的差异。第3节详细介绍了本文提出的TRIMOON模型。第4节提供了验证实验的设置和实验结果的观察和分析。第5节讨论了建模中需要解决的一些问题。最后一部分是结束语，指出了未来的研究方向。 相关工作Based on traditional machine learning Castillo等[19]从用户的传播行为角度出发，构建了内容、话题、传播、行为等特征集，并通过SVM[20]、J48[21]等机器学习算法进行验证，最终达到89%的分类准确率。实验验证了引入上述四类特征的有效性。 Ruchansky等[22]提出了一种自动假新闻检测器CSI (Capture, Score, and Integrate)。它由三个模块组成:捕获、评分和集成。检测器通过使用与传入新闻相关的三个特征来预测假新闻:文本、响应和来源。该模型由三个模块组成。第一个算法提取新闻文章的时间表示。第二个模块表示用户的行为并对其进行评分，最后一个模块使用前两个模块的输出并使用它们进行分类。他们的实验表明，CSI的准确性有所提高。Reis等人[23]针对该任务提出了一种新的特征集，并将其应用于现有的自动检测模型，并通过实验验证了新特征集的有效性。此外，假新闻的目的之一是操纵人们的观点，因此在一些工作中，情绪信息[24]也被认为是一个特征[25,26]。 显式特征很容易解释，但不一定是机器学习模型学习的最佳数据。最近，一些研究工作开始探索假新闻的隐含特征。Guo等[27]根据新闻数据，基于用户画像构建特征，纳入用户行为、可信度、可靠性等隐式特征。Wu等[28]提出了新闻话题类型和情感特征，结合消息传播间隔特征，使用随机行走[29]核函数的支持向量机算法进行分类，在微博数据集上取得了很好的效果。 Based on deep learning and multi-modal 随着新闻内容变得越来越复杂，仅仅依靠人工特征是不够的。此外，随着深度学习算法的普及，许多研究人员已经开始使用基于深度神经网络的模型来完成假新闻检测任务。挖掘出比传统机器学习中使用的人工特征更重要、更容易学习的特征。同时，通过深度神经网络将多模态信息引入到假新闻检测中。 Jin等人[32]提出了一种基于注意力的递归神经网络(att-RNN)，该网络融合了来自文本、视觉和社会背景的信息。Wang等[33]提出了一种事件对抗神经网络(Event Adversarial Neural Network, EANN)，将事件分类任务引入到对抗学习中，引导模型学习与事件无关的文本模态和图像模态特征，具有更好的泛化性能。Khatter等[34]提出了一种端到端的多模态变分自编码器(MVAE)网络，该网络由编码器、解码器和假新闻检测模块三个主要组成部分组成，利用编码器-解码器结构构建多模态新闻的特征表达。 以上方法在检测具有多模态信息的假新闻方面是有效的，但由于缺乏足够的事实知识，无法充分理解多模态新闻事件的深层语义。 为了解决这一问题，Zhang等[35]提出了一种新的多模态知识感知事件记忆网络(MKEMN)，该网络利用多模态知识感知网络(MKN)和事件记忆网络(EMN)作为社交媒体谣言检测的构建模块。从外部知识图谱中提取文本实体对应的概念知识，并将其集成到多模态表示中，以获得更高的语义理解能力。 Wang等[36]提出了一种新的知识驱动的多模态图卷积网络(KMGCN)，该网络通过对文本信息的联合建模来建模语义表示，并将知识概念和视觉信息集成到统一的假新闻检测框架中。事实上，图像的可用性也是多模态融合模型中需要考虑的问题。 一些工作考虑将文本与图像中出现的实体对齐，以区分图像和文本之间的不一致，从而提高检测性能[37,38]。随后，Li等人提出了一个以实体为中心的多模态学习框架，该框架通过实体对齐和以实体为中心的特征聚合两个模块学习新的特征表示来训练分类器[39]。Chen等人提出了一种跨模态歧义学习模型，用于单模态和多模态特征的动态融合，歧义大时以单模态特征为主，歧义小时以跨模态特征为主[40]。 本文从图像和文本一致性的角度提出了一种新的多模态融合方法。与实体级一致性不同[37,38]，我们考虑图像与新闻文本的整体语义一致性[16,40]。在我们的方案中，我们突出文本模态作为新闻的主导信息，同时基于不一致性适当融合图像模态信息，而[16]直接将一致性与类标签对齐，[40]从数据集整体层面学习图像和文本模态的一致性表示并在此基础上进行加权融合。在我们的模型中，无论图像信息与文本内容是否一致，都不会取代文本模态特征作为分类的主导特征，这也符合我们人类阅读和判断新闻真假的习惯。 方法给定一个集合𝐷&#x3D;{(𝑥𝑖,𝑦𝑖),𝑖&#x3D; 1,…,𝑁}，𝑁新闻条目,每个新闻𝑥𝑖有标签𝑦𝑖∈{0,1}，𝑥𝑖包含𝑥𝑇𝑖和𝑥𝑉𝑖,哪里𝑥𝑇𝑖新闻中的文本,𝑥𝑉𝑖新闻图片。 我们提出的模型将利用训练实例学习一个从特征空间𝑋到标签空间𝑌的映射函数∶𝑋→𝑌，然后利用其特征向量来预测实例的标签向量。我们模型的输入是每个新闻条目的文本和图像。使用BERT和VGG (Visual Geometry Group)网络分别学习文本特征和图像特征。 在我们的工作中，我们认为在检测假新闻时，文本模型是基础，图像信息是辅助。 同时，图像-文本不一致是双模融合中必须考虑的问题。因此，在我们提出的方法中，在每次融合之前，图像信息将经过一个门结构进行信息选择。在多模态特征融合之前，对VGG-19馈送的图像特征进行第一次不一致性度量。然后，我们使用共同关注来捕捉文本特征和过滤后的图像特征之间的关系。在第二次融合中，我们通过另一个不一致性测量门来控制第一次融合后的结果，然后与文本信息重新融合。最后，通过具有全连接层的BiLSTM网络获得预测结果。输出是该新闻的标记(true或false)。所提出的模型整体结构如图2所示。该模型包括: (1)多模态特征提取模块; (2)多模态特征融合模块; (3)分类模块。 The multi-modal feature extraction module 我们使用BERT作为文本编码器。BERT模型的初始输入是一组句子𝑆&#x3D;{𝑠1，𝑠2，…，𝑠𝑚}，𝑠𝑚表示𝑚th句子，其中𝑚∈𝑀;句子𝑠可以表示为一组字符𝑠&#x3D;{𝑤1，𝑤2，…，𝑤N}，𝑤𝑛表示句子中的𝑛th字符。在我们的任务中，𝑆&#x3D;𝑥𝑇𝑖，即𝑖th新闻的文本部分。BERT输入向量由词嵌入向量、段嵌入向量和位置编码向量组成。 词嵌入向量是每个词的向量表示，BERT可以以句子对的形式进行训练，并使用分割的嵌入向量来识别句子。在位置编码向量中，BERT使用学习到的位置编码来识别每个单词的位置信息。然后，BERT模型使用Transformer[44]编码器构建多层双向网络，该网络由多层Transformer编码器堆叠。编码器的每一层由多头自关注子层和前馈神经网络子层组成。 𝑇𝑜𝑘𝑒𝑛𝐵𝑒𝑟𝑡是BERT模型的token-level（标记）输出序列。𝑆𝑒𝑛𝑡B𝑒𝑟𝑡为BERT模型的句子级输出; 𝑚𝑡是输出文本特征。 对于新闻分集，提取图像特征的过程可以用方程表示。(4) -(6)。 The inconsistency measurement module不一致性测量模块 该模块通过测量文本和图像之间的语义相似度来评估文本和图像的不一致性。与那些在实体层面衡量一致性的作品不同[37,38]，我们从整体一致性的角度进行思考。 在上一模块中，我们使用BERT和VGG-19学习了文本和图像的特征表示。然后我们在线性层上应用sigmoid函数，其中我们将特征表示连接为输入，以测量它们之间的不一致性。然后，我们采用乘法门来获得加权图像模态表示。在这里，该模块旨在衡量同一新闻的图像和文本的语义一致性，并通过以下门机制进一步控制可以融合的信息程度。可以使用多种方法来度量语义一致性。通过实验比较，我们发现对于我们的数据集，一个简单的线性层是一个更好的选择。 特征融合 我们的多模态特征融合主要由两个组件完成:一个由两个并行的共同注意块组成的共同注意层[46]和一个基于门的融合模块。图3给出了共注意块的结构，在Co-Attention块中，它的查询和键(&#x3D;value)来自不同的地方，即，如果查询来自文本，则键(&#x3D;value)来自图像，反之亦然。 对于图像模态，计算过程如下: 对于文本模态，通过相同的计算过程，我们可以得到文本模态特征。通过将共同注意块A和共同注意块B并行排列，并将它们组合成一个共同注意层。共同关注块A使用文本特征作为查询和图像特征为键，共同关注块B使用图像特征作为查询，文本特征作为键。这样就实现了文本和图像之间信息的交互学习。 基于门的融合模块是第二融合。与已有工作中的共关注叠加分量[47]不同，我们的第二次融合再次基于门机制对特征信息流进行控制，以获得更可靠的特征表示。Wu等[47]在多模态信息重要性相等的前提下，使用了特征信息的迭代融合。但是，我们认为图像模态(及其融合特征)只是辅助信息，其权重需要通过逐步向前融合时的一致性程度来衡量。 融合过程如式所示。(13) -(16)。 结论与讨论 通过检测文本模态与图像模态的不一致性，使我们的模型在融合多模态信息时能够学习到更合理的特征表示，从而提高检测性能。此外，在我们的融合模型中，文本形态的主导地位并没有被图像特征所掩盖，就像文本信息是一般新闻的主导信息表示一样。 事实上，当图片与文字不一致时，发布者有可能使用不准确的(挪用的，历史的)图片作为伪造的证据来支持新闻文本中的结论。这是基于不一致信息构造特征融合表示的原因之一。然而，通过对上述实验结果的分析，我们也发现我们的模型对于图像-文本一致性检测仍然处于粗粒度水平，受限于缺乏对图像的语义理解。因此，细粒度图像理解和图像-文本一致性检测仍然是一个挑战。此外，当图像与文本高度一致时，特征失去了多样性，如何更有效地利用或增强文本模态的特征是另一个具有挑战性的问题。 本文提出了一种新的多模态融合方法——基于图像-文本不一致性的两轮多模态融合方法来检测假新闻。首先，分别通过BERT和VGG预训练模型获得文本模态和图像模态的向量表示。然后通过不一致性测量方法计算检测任务的图像模态权重，并进行第一次图像-文本融合以获得融合特征。这些特征不直接输入到分类器中，而是与文本模态的主要特征再次融合，得到最终的特征表示，最后使用分类器进行分类和检测。与其他方法相比，我们的方法有两个明显的优点:(1)在初始融合之前，通过不一致权值对信息进行过滤，从而控制了不真实图像对内容的误导;(2)通过二次融合机制，加强文本模态特征的主导地位，同时兼顾图像模态信息，达到更好的检测性能 在今后的研究中，可以在以下几个方面开展进一步的工作。首先，设计基于视觉语言预训练语言模型的图像文本表示和细粒度一致性检测方案，其次，在标签表示、实体识别和事件识别等辅助方法的基础上，增强基于文本特征和不一致性的多模态融合学习能力。第三，考虑不同粒度的语义不一致度量方法。通过使用上述方法，可以更好地整合图像和文本内容，获得更好的融合特征，实现对假新闻的高效准确检测。","categories":[{"name":"阅读笔记","slug":"阅读笔记","permalink":"https://sandra-feng.github.io/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"学习记录","slug":"学习记录","permalink":"https://sandra-feng.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}]},{"title":"多模态相关综述","slug":"多模态相关综述","date":"2023-08-30T06:04:21.000Z","updated":"2023-08-30T06:04:21.000Z","comments":true,"path":"多模态相关综述.html","link":"","permalink":"https://sandra-feng.github.io/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9B%B8%E5%85%B3%E7%BB%BC%E8%BF%B0.html","excerpt":"","text":"多模态相关综述多模态信息处理技术打破计算机视觉、语音与声学、自然语言处理等学科间的壁垒，是典型的多学科交叉技术。 多模态核心技术又分为：多模态表示，多模态融合（Ｆｕｓｉｏｎ）、多模态转换（Ｔｒａｎｓｌａｔｉｏｎ）、多 模 态 对 齐 （Ａｌｉｇｎｍｅｎｔ）和 模 态 协 同 学 习（Ｃｏ-ｌｅａｒｎｉｎｇ）类。 从自然语言处理的角度出发，2022年—2020年左右，在国际自然语言领域的顶刊顶会上，关度较高的几个多模态应用如下： 视觉语言生成是给定一个图像生成一段语言描述，或者给定一段话生成一幅图，也包括视频描述任务等 视觉问答典型的例子是在商品检索中如图: 多模态摘要又分为视频会议摘要、教学视频摘要、多模态商品摘要、多模态新闻摘要。 多模态对齐研究多个模态不同颗粒度元素间的对齐关系。比如，在大规模图像－词汇对齐的多模态语料库上训练的预训练语言模型可增强其对自然语言的理解能力。 等应用这里不再多讲，感兴趣可以去知网查看[1]吴友政,李浩然,姚霆等.多模态信息处理前沿综述：应用、融合和预训练[J].中文信息学报,2022,36(05):1-20.以上内容均来源于此。 用transformer多模态学习的综述人工智能（AI）的最初灵感是模仿人类的感知，例如看到、听到、触摸、嗅闻等。通常，一个模式通常与特定的传感器相关联，并创建独特的通信通道，例如视觉和语言 [1]本质上，多模态AI系统需要摄取、解释和理解多模态信息源才能实现与人类水平相当的感知能力。多模态学习（MML）是一种构建可以从多模态数据中提取和关联信息的AI模型的通用方法 [1]。 本调查重点关注使用Transformer进行多模态学习（如图1所示），并受到其固有优势和可扩展性（例如，语言，视觉，听觉）建模不同模态（例如，语言翻译，图像识别，语音识别）和任务（例如，语言翻译，图像识别，语音识别）的启发。 背景MML[1]，[60]，[61]是近几十年来重要的研究领域;早期的多模态应用——视听语音识别在20世纪80年代进行了研究[62]。MML是人类社会的关键。我们人类生活的世界是一个多模态环境，因此我们的观察和行为都是多模态的[63]。例如，AI导航机器人需要多模态传感器来感知现实环境[64]，[65]，[66]，如摄像头，LiDAR，雷达，超声波，GNSS, HD Map，里程表。此外，人类的行为、情绪、事件、动作和幽默都是多模态的，因此各种以人为中心的MML任务被广泛研究，包括多模态情绪识别[67]、多模态事件表示[68]、理解多模态幽默[69]、基于脸-身体-声音的视频人聚类[70]等。 感谢互联网的发展和近年来智能设备的大规模应用，越来越多的多模态数据正在通过互联网传输，因此出现了越来越多的多模态应用场景。在现代生活中，我们可以看到许多多模态应用，包括商业服务（例如电子商务&#x2F;商品检索[71]，视觉和语言导航(VLN) [72]，[73]，[74]，[75]，[76]）、通信（例如唇读[77]，手语翻译[28]，[29]）、人类-计算机交互[78]、医疗AI[79]、监控AI[81]等。 此外，在深度学习的时代，深度神经网络极大地促进了多模态学习（MML）的发展，Transformer架构是一个具有竞争力的架构家族，为MML带来了新的挑战和机遇。特别是，最近大型语言模型的成功和它们的跨模态衍生模型（例如[82]，[83]，[84]，[85]，[86]）进一步证明了Transformer在多模态基础模型中的潜力。 里程碑 受到Transformer的成功启发，VideoBERT [7]是一项突破性的工作，是第一个将Transformer扩展到多模态任务的研究。VideoBERT证明了Transformer在多模态背景下的巨大潜力。在VideoBERT之后，许多基于Transformer的多模态预训练模型（例如ViLBERT [102]，LXMERT [103]，VisualBERT [104]，VL-BERT [105]，UNITER [106]，CBT [107]，Unicoder-VL [108]，B2T2 [109]，VLP [110]，12-in-1 [111]，Oscar [112]，Pixel-BERT [113]，ActBERT [114]，ImageBERT [115]，HERO [116]，UniVL [117]）已经成为机器学习领域的研究热点。 在2021年，CLIP [9]被提出。它是一个新的里程碑，通过将多模态预训练应用于分类，将其转化为一个检索任务，使得预训练模型能够解决zero shot识别。因此，CLIP是一个成功的实践，充分利用了大型多模态预训练，实现了zero shot学习。最近，CLIP的概念 further研究，例如基于CLIP的zero shot语义分割 [118]，CLIP-TD [119]，ALBEF [121]和CoCa [122]。 数据集 多模态数据集的新趋势; （1）数据规模更大。各种最近发布的 datasets 包括 Product1M、Conceptual12M、RUC-CAS-WenLan（30M）、HowToVQA69M、HowTo100M、ALT200M 和 LAION-400M 等，数据规模都在百万级别。 （2）视觉、文本和音频等多模态数据集正在不断涌现，包括更多的多样模态，例如：Pano-AVQA：第一个大型空间和音频视觉问题回答数据集，适用于360度视频，YouTube-360（YT-360）[145]（360度视频），AIST++[146]（一个新的多模态数据集，包括3D舞蹈动作和音乐），Artemis [147]（用于视觉艺术的情感语言），MultiBench [148]（包括10种模态的数据集）。 （3）更多场景。除了常见的字幕和问答数据集外，还研究了更多应用场景，例如：CIRR [149]（现实图片），Product1M [137]（产品1M），Bed and Breakfast（BnB）[150]（视觉-语言导航），M3A [151]（金融数据），X-World [152]（自动驾驶）。 （4）任务更具挑战性。除了简单的任务之外，还提出了更多抽象的多模态任务，例如：MultiMET（一种多模态数据集，用于隐喻理解），Hateful Memes（带有仇恨言论的多模态数据集）。 （5）指令式视频变得越来越流行，例如：烹饪视频 YouCookII [155]。将指令与某人执行任务的视频对齐是一个强大的预训练前缀任务[7]，[156]。预训练任务是在解决它们的基础上设计的，以迫使模型学习表示。 类似于其他深度神经网络架构，Transformer 也具有数据需求大的特点。因此，其高容量模型和多模态大数据基础共同推动了基于多模态的机器学习的发展。例如，大数据使 VLP Transformer 模型具有零散学习能力。 multimodal transformertansformer的基础架构不再多讲，直接看多模态。 给定一个任意模态的输入，用户只需要执行两个主要步骤：1）对输入进行标记，2）选择一个表示标记的嵌入空间，然后将数据输入到Transformer中。在实践中，标记输入和选择嵌入空间对于Transformer来说至关重要，但具有很多灵活性。例如，对于图像，标记和嵌入的解决方案不是唯一的。用户可以在多个粒度级别上进行标记和嵌入选择——粗粒度（coarse-grained）和细粒度（fine-grained）。 这里标记是tokenizing，”Tokenizing input”是将输入文本分割成单个的标记或词语的过程。这可以通过使用空格、标点符号或其他规则来实现。例如，将句子”Hello, how are you?”分割成标记[“Hello”, “,”, “how”, “are”, “you”, “?”]。”Embedding”是将这些标记转换为向量表示的过程。向量表示可以捕捉到标记之间的语义和关系。常见的方法是使用预训练的词嵌入模型，如Word2Vec、GloVe或BERT，将每个标记映射到一个固定长度的向量。 在多模态Transformer中,跨模态交互(例如融合、对齐等)本质上是由自注意力和其变体处理的。因此,在本节中,我们将从自注意力的设计角度回顾Transformer的主要多模态建模实践,包括(1)早期的求和(词级别,加权),(2)早期的连接(跨模态连接),(3)层次注意力(多流到一流),(4)层次注意力(一流到多流),(5)跨模态注意力(跨模态关注),和(6)跨模态注意力的连接。请参阅表2和图2 上述所有多模态交互的自关注变体都是模态通用的，可以应用于灵活的策略和多粒度任务。具体来说，这些交互可以灵活地组合和嵌套。例如，在两流解耦模型[191]中，多个交叉注意流用于分层注意(一流到多流)，Eq. 11中的tf2和tf3通过Eq. 12中定义的交叉注意实现。此外，它们可以扩展到多个(≥3)模式。TriBERT[183]是视觉、姿势和音频的三模态交叉注意(共注意)，其中给定查询嵌入，其键和值嵌入是来自其他模态的连接。在[189]中，交叉注意串联被应用于三种模式(即语言、视频和音频)。 Transformers for Multimodal PretrainingTransformer也被广泛用于多模态预训练。最近的研究表明，如果在基于大规模多模态语料库Transformer的模型上进行预训练[7]，[102]，[103]，[104]，[105]，[106]，[110]，在大范围的多模态下游任务中明显优于其他竞争对手，并且实现了零样本泛化能力。这些优势使得基于transformer的多模态预训练成为一个热门话题，主要有两个方向，即针对不可知性下游任务的一般性预训练(章节4.1.1)和针对特定下游任务的目标导向预训练(章节4.1.2)。 我们关注这些关键点:(1)正在出现哪些趋势?(2)在预训练过程中，跨模态交互在哪里&#x2F;如何发生?(3)如何梳理和理解预训练托词目标?他们如何使用transformer学习跨模态交互? 在基于Transformer的多模态预训练中，预训练任务&#x2F;目标通常也被称为预训练任务&#x2F;目标。到目前为止，已经研究了许多预训练任务，例如： 遮蔽语言建模（MLM）[137] 遮蔽图像区域预测&#x2F;分类（也称为遮蔽对象分类，MOC）[137]，[190] 遮蔽区域回归（MRR）[115] 视觉-语言匹配（VLM）(例如，图像-文本匹配(ITM) [188]，图像文本匹配(ITM)，短语区域对齐(PRA) [204]，词区域对齐(WRA) [106]，视频字幕匹配(VSM) [116] 遮蔽帧建模（MFM）[116]，帧序建模（FOM）[116]，下一句预测（NSP）[4]，[102]，[190]，遮蔽句子生成(MSG) [191]，遮蔽组建模(MGM) [188]，前缀语言建模(PrefixLM) [199]，视频有条件遮蔽语言模型(也称为视频受控遮蔽语言模型)[117]，有条件文本遮蔽图像模型(也称为有条件图像遮蔽语言模型)[117]，视觉翻译语言建模(VTLM) [206]，以及图像有条件遮蔽语言建模(也称为图像关注遮蔽语言建模)[207]。这些下游任务无关的预训练预处理可以是可选的，而下游任务目标可以直接训练，这在第4.1.2节中将会讨论。表3提供了基于Transformer的多模态预训练中常见的预训练任务。 在多模态变压器的实践中，上述与下游任务无关的预训练是可选的，不是必须的，针对下游任务的预训练也被广泛研究[150]，[190]，[208]，[211]。主要原因包括:(1)受现有技术的限制，很难设计出一套高度通用的网络架构、借口任务和语料库，适用于所有不同的下游应用。(2)各种下游应用之间存在不可忽略的差距，如任务逻辑、数据形式等，使得从预训练到下游应用的转移变得困难，因此，大量的下游任务仍然需要量身定制的预训练来提高性能。 挑战与设计融合一般来说，MML transformer主要在三个级别融合跨多种模式的信息:输入、中间表示、预测。 我们注意到简单的基于预测的后期融合[247]，[248]在多模态 transformer中较少采用。考虑到学习更强的多模态上下文表示的动机和计算能力的巨大进步，这是有道理的。为了增强和解释MML的融合，探索模式之间的相互作用和测量融合[249]将是一个有趣的探索方向。 对齐 可转移性 可迁移性是Transformer基于多模态学习的一个主要挑战,涉及如何将模型在不同的数据集和应用中进行转移。数据增强和对抗扰动策略有助于提高多模态Transformer的泛化能力。VILLA是一种两阶段策略(任务无关对抗预训练,然后针对任务的对抗微调),可以提高VLP Transformer。 在实践中,训练数据和实际数据的分布差异是显着的。例如,监督数据样本(良好标注、对齐)在实际应用中成本很高。因此,如何将预训练的多模态Transformer迁移到弱对齐的测试平台上是一个具有挑战性的问题[137]。CLIP是一种鼓舞人心的解决方案,通过学习共享的多模态嵌入空间,将知识从一个模态迁移到另一个模态,实现零散转移。CLIP的主要灵感是,预训练的多模态(图像和文本)知识可以通过使用提示模板“一张{标签的照片}”来将在训练和测试数据之间传递分布差距。 过拟合是一个向量转移的主要障碍。多模态Transformer在训练过程中可能会过度拟合数据集,由于其建模能力很大。一些最近的实践探索了如何将或acle模型在无噪声数据集上训练的数据迁移到真实数据集。例如,Kervadec等人[272,273]研究了VQA中的可迁移推理模式,并表明对于LXMERT[103]&#x2F;BERT-like推理模式,可以从理想数据集中部分转移到真实数据集中。 跨任务差距是另一个向量转移的主要障碍。根据[208],[274],由于不同的推理和输入-输出工作流程,例如如何将多模态数据集用于语言预训练模型的微调,是一个具有挑战性的问题。在实际应用中,有时需要处理未映射的单模态数据,这通常是由于缺失模态而导致的。一种解决方案是使用知识蒸馏,例如,在Transformer中从多模态到单模态注意力[275],从多个单模态Transformer教师到共享Transformer编码器[276]。在多模态任务和生成任务之间存在巨大的差距。正如[208]中所讨论的,基于BERT的编码器- only多模态Transformer(例如,VideoBERT [7], CBT [107])需要分别对生成任务训练解码器。这可能导致预训练-微调差异不利于泛化。最近,越来越多的研究探讨了这个问题,例如,GilBERT是一种用于有监督任务的生成VLP模型。 跨语言差距也应该考虑,以确保Transformer基于多模态学习的可转移性,例如,英语到非英语多模态上下文下的通用跨语言通用学习[206],[277]。 效率多模态Transformer有两个主要效率问题: (1) 由于具有较大的模型参数容量,它们对数据量有很高的需求,因此依赖于大规模训练数据集。 (2) 由于自注意力机制,它们的计算复杂度随输入序列长度的增长而呈指数级增长,这会导致在多模态环境中计算爆炸。这些问题是相互依存的,应该一起考虑。 为提高多模态Transformer的训练和推理效率,近年来提出了各种解决方案,以使用更少的训练数据和&#x2F;或参数。主要想法可以总结为以下几点: (1) 知识蒸馏。将训练的大型Transformer模型中的知识蒸馏到较小的Transformer模型中[93]。Miech等人[278]从慢模型(基于早期连接的Transformer)到快模型(基于独立 dual branch 的Transformer)进行了蒸馏。 (2) 简化和压缩模型。去除组件以简化模型管道。以VLP Transformer模型为例,使用对象检测模型进行简化会花费很多代价,因此可以使用视觉输入的无约束方式,例如,E2E-VLP[271]和ViLT[192]。DropToken通过随机丢弃输入序列的一部分视频和音频token来降低训练复杂度。DropToken可以被视为一种实现dropout或对抗训练的方法。权重共享也是常见的多模态Transformer模型简化的方法之一。Wen等人[279]提出了一种基于视觉和文本编码器的权重共享Transformer,以对齐文本和图像。Lee等人[280]提出了一种新的参数共享方案,基于低秩近似。 (3) 不对称网络结构。为不同的模块分配不同的模型容量和计算复杂度,以节省参数。参考文献[192]中的图2。 (4) 提高训练样本的利用率。Liu等人[281]通过充分利用较少的样本数据来训练简化LXMERT模型。Li等人[282]使用更少的数据来训练CLIP模型,通过充分利用模型的潜在自监督信号,来自模型的距离较近的相似对之间使用最近邻监督。 (5) 压缩和剪枝模型。搜索多模态Transformer模型的最优子结构&#x2F;网络,例如,使用VLP Transformer模型进行彩票抽奖[283]、Lottery Tickets策略。 (6) 优化自注意力。Transformer模型在输入序列长度上需要大量的时间和内存,一个可能的解决方案是优化O(N2)的复杂度,例如,Child等人[286]提出的稀疏注意力矩阵的稀疏分解。Transformer LS[287]是既高效又高效的Transformer模型,具有线性计算和内存复杂度。 (7) 优化基于多模态交互&#x2F;融合的自注意力。Nagrani等人[175]提出了Fusion via Attention Bottlenecks(FSN)来提高多模态交互&#x2F;融合的性能。FSN通过少数 bottleneck latent 传递消息,因此模型需要对每个模态进行最必要的信息提取。这种策略利用了 bottleneck 作为桥梁,不仅提高了融合性能,而且降低了计算成本。 (8) 优化其他策略。使用最优策略执行常见的Transformer基于多模态交互。考虑到自注意力的平方复杂度,使用基于早期连接的多模态交互来同步融合多个模态的输入是昂贵的。Yan等人[288]提出了一种有效的解决方案,即在序列长度升序顺序中,逐个将两个相邻视图的信息融合起来。这是一种贪心策略。 多样性由于多模态学习任务的多样性和模态数量,通用性对于多模态Transformer模型来说是一个重要的问题。在实践中,大量的最新尝试都试图尽可能地统一各种模态和任务,以便处理各种任务和数据。理想情况下,统一的多模态Transformer模型应该可以兼容各种数据(例如,对齐和未对齐的,单模态和多模态的)和任务(例如,监督和无监督,单模态和多模态,有监督和无监督,推理和生成)。同时,这些模型应该具有能够在 小样本或甚至零样本情况下进行一般化的能力。因此,目前为了解决通用性问题,采取的临时解决方案只是初步的探索。 目前,统一模型的尝试主要包括以下两种: (1)统一输入&#x2F;任务的管道。正如第5.3节所述,在实际场景中,由于缺失模态,多模态Transformer模型需要处理单模态数据。将多模态知识蒸馏到适应单模态数据和任务的模型中是一种成功的方法[275],[276]。 (2)统一理解和生成管道。通常情况下,理解和生成任务都需要Transformer编码器,而生成&#x2F;生成任务还需要Transformer解码器。现有尝试使用多任务学习来将理解和生成任务合并进行训练,其中两种类型的工作流程共同训练。从模型结构上看,这通常包括: (a)编码器+解码器,例如E2E-VLP。 (b)分离的编码器+跨编码器+解码器,例如UniVL和CBT。 (c)单解码器&#x2F;联合编码器,例如VLP。 (d)双流解码器设计,例如两个流解码器。 虽然上述尝试提供了一些初步的探索,但也存在一些明显的问题和限制,至少包括: (1)由于模态和任务之间的差距,通用模型应该考虑泛化能力和成本之间的平衡。统一模型通常会导致更大的或更复杂的模型配置,而针对特定模态或任务,一些组件可能是冗余的。 (2)多任务损失函数会增加训练的复杂度。如何正确地共同训练多个目标以及有效地优化它们是一个具有挑战性的问题,因为不同的目标通常应该在不同的策略上进行优化。 讨论设计一个通用的MML模型,在所有单模态和多模态下游任务中同时具有不同的特征是一个非常大的挑战，Transformer模型(如[9]、[199]、[263]等)在特定MML任务上表现优异,但它们仅被设计为针对特定任务的模型(如[137]、[142]、[212]、[249]、[260]、[261]、[265]、[266]等)。令人鼓舞的是,一些最近的研究关注于在模态无关网络设计方面实现通用模态学习( modality-agnostic network design )[3]和更通用架构设计[307]、[308]、[309],并期望这会引发更多的研究调查。因此,我们不应对模型的设计空间进行深入探索,而是寻求对MML模型的行为进行深入理解和解释,即使不同模态之间的交互和协同效应本质上是复杂和可能不一致的[249]。 对于更细粒度的MML,发现跨模态的潜在语义对齐对于模型的成功至关重要。一种直观的策略是利用预提取的语义部分(如物体)来支持MML[103]、[104]、[105]、[106]、[112]、[204]、[310]。然而,这不仅复杂且容易出错,而且计算代价昂贵[207]。最近,提出了一些解决方法,包括随机采样[113]、学习概念词典[203]、联合学习区域检测器[271]和掩码预测前的表示对齐[263]。考虑到MML训练数据的规模,探索这一方向需要极大的计算成本,而且据富有的研究团队来说,更加划算。理想的方法是让MML在模态之间自动产生细粒度的语义对齐,这值得未来仔细研究。 随着学习规模的指数扩展,训练数据变得不可避免地嘈杂和异质化[9]、[199]、[263]。最近,已经证明解决噪声问题非常重要[263]、[309]。另一个相关的问题是训练策略,例如,训练阶段数量是否优于常见的单阶段策略[115]。此外,由于多模态数据的输入更长,Transformer的平方复杂度变得更为尖锐。尽管对高效变体进行了大量研究[49],但为MML进行专门性的效率研究仍然是不足的,并需要更多的调查。 以下是Transformers在多模态机器学习方面的优势的概述: (1) Transformers可以编码隐含知识[32]。 (2) 多头带来了多个建模子空间,可以进一步增强模型的表现能力。理想情况下,训练后多个头都是好的,并且不同。这是集成学习的良好实践。 (3) Transformers固有的全局聚合特性感知非局部模式。 (4) 由于大型模型的能力,Transformer模型通过在大型语料库上进行有效的预训练来处理具有挑战性的领域缺口,例如语言和视觉[294]。 (5) Transformers可以表示输入作为一个图,与更多的模态兼容,例如表格和SQL。 (6) 对于建模序列和序列模式(如时间序列),Transformer模型在训练和&#x2F;或推理中的效率比基于RNN的模型更好,得益于它们在训练和&#x2F;或推理中的并行计算。Transformers天生对平移变换不变,因此是点云学习的理想工具[164]。 (7) 分词使得Transformers灵活地组织多模态输入,正如第3.1.1节中所述。 论文选自Multimodal Learning with Transformers:A Survey","categories":[{"name":"阅读笔记","slug":"阅读笔记","permalink":"https://sandra-feng.github.io/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"学习记录","slug":"学习记录","permalink":"https://sandra-feng.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}]},{"title":"Bert微调及BERT变体","slug":"Bert微调及BERT变体","date":"2023-08-01T06:04:21.000Z","updated":"2023-08-01T06:04:21.000Z","comments":true,"path":"Bert微调及BERT变体.html","link":"","permalink":"https://sandra-feng.github.io/Bert%E5%BE%AE%E8%B0%83%E5%8F%8ABERT%E5%8F%98%E4%BD%93.html","excerpt":"","text":"Bert微调及BERT变体1、BERT预训练作用 学习通用语言表示：通过在大规模无标签语料上进行预训练，BERT可以学习到丰富的、通用的语言表示。这些语言表示可以捕捉到词汇、句法和语义等不同级别的信息，从而能够更好地理解和表示自然语言的含义。 上下文理解：BERT通过双向编码方式对文本进行建模，可以有效地理解上下文中的依赖关系和语义信息。模型的每个位置可以同时考虑其前后文的上下文信息，而不仅仅是局限于当前位置。 词义消歧：BERT预训练模型对于词义消歧具有优势。通过预训练，模型可以学习到多义词的各种上下文语境，进而能够更好地理解和区分同一词在不同上下文中的含义。 迁移学习：预训练的BERT模型具有广泛的语言理解能力，这使得它可以作为下游任务的初始模型进行迁移学习。通过微调预训练模型，可以在特定任务上节省大量的数据和时间，同时获得更好的性能。这对于具有有限数据集的任务和资源受限环境中的应用特别有用。 2、什么是微调？ 链接：(141条消息) fine-tuning（微调）的理解_好耶OvO的博客-CSDN博客 3、什么是冻结层 冻结层指该层不加入网络训练，该层参数不会更新。 怎么固定住某些网络 4、BERT变体 BERT的动态掩码机制能够学习到更好的上下文表示和更准确的单词表示，提高对单词的理解能力，增加模型的鲁棒性和泛化能力。NSP（Next Sentence Prediction）任务通过预测下一句子的概率，模型需要学习句子之间的语义关系和连贯性。这使得模型能够更好地理解句子中的上下文信息，从而提高对句子语义的理解和表示能力。这对于某些任务，如文本匹配、推理和问答等需要考虑句子之间关系的任务特别有用。模型可以利用学到的句子级别的语义信息在这些任务中更好地获取句子之间的语义匹配和推理能力。 (141条消息) 一起来学习BERT常见的几个变体_bert变体_愤怒的可乐的博客-CSDN博客","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"https://sandra-feng.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"}],"tags":[{"name":"学习记录","slug":"学习记录","permalink":"https://sandra-feng.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}]},{"title":"具有自适应微调策略的分层BERT，用于文档分类","slug":"具有自适应微调策略的分层BERT，用于文档分类","date":"2023-07-29T06:04:21.000Z","updated":"2023-07-29T06:04:21.000Z","comments":true,"path":"具有自适应微调策略的分层BERT，用于文档分类.html","link":"","permalink":"https://sandra-feng.github.io/%E5%85%B7%E6%9C%89%E8%87%AA%E9%80%82%E5%BA%94%E5%BE%AE%E8%B0%83%E7%AD%96%E7%95%A5%E7%9A%84%E5%88%86%E5%B1%82BERT%EF%BC%8C%E7%94%A8%E4%BA%8E%E6%96%87%E6%A1%A3%E5%88%86%E7%B1%BB.html","excerpt":"","text":"具有自适应微调策略的分层BERT，用于文档分类中科院一区 摘要预训练语言模型(PLMs)已经取得了令人印象深刻的成果，并已成为各种自然语言处理(NLP)任务的重要工具。然而，当文档长度超过PLM的最大可接受长度时，将这些PLM应用于文档分类存在一个限制，因为在这些模型中会截断多余的部分。如果关键字在截断的部分，则模型的性能下降。为了解决这个问题，本文提出了一种带有自适应微调策略的分层BERT (HAdaBERT)。它由基于bert的局部编码器模型和基于注意力的门控记忆网络作为全局编码器组成。与直接截断文档的现有plm相比，所提出的模型使用文档的一部分作为区域，将输入文档划分到几个容器中。这允许本地编码器提取每个容器中的有用信息，并由全局编码器根据其对分类的贡献进行组合。为了进一步提高模型的性能，本文提出了一种自适应微调策略，该策略动态地决定需要微调的BERT层，而不是对每个输入文本的所有层进行微调。在不同语料库上的实验结果表明，该方法在文档分类方面优于现有的神经网络。 引言文件中的重要信息可能会在不同的地方分发。句子之间的语义关系更加复杂和模糊，使得文档分类成为一项具有挑战性的任务。cnn在计算机视觉方面取得了成功，也被用于文档分类。假设文档中的每个token对分类的贡献不相等，自动对齐文本和强调重要token的过程、自注意力、动态路由已经被提出进一步提高cnn、GRU和LSTM网络的性能。此外，还提出了分层关注网络[15]用于文档分类，在句子级和文档级执行语义建模，然而这可能会导致token在句子上下文中的句法依赖性。 最近，预训练语言模型： BERT、ALBERT [17] 、 RoBERTa [18]成功地完成了各种NLP任务。在处理下游任务时，这些plm消除了从头构建模型的需要，并且可以采用transformers，self-attention mechanisms利用迁移学习学习高质量的文本语境表征。通常，首先向plm提供大量未注释的数据，然后通过屏蔽语言模型或下一个句子预测进行训练，以学习各种单词的用法以及该语言的一般编写方式。然后，模型被转移到另一个任务，在那里它们被馈送到另一个较小的任务特定数据集。对于文档分类，输入序列可以很长，但BERT模型的最大输入长度在大多数情况下是有限的[20]。一旦文档超过预设的最大输入长度，这些文档的多余部分将被直接截断。最佳短语和强烈推荐短语中的强烈主观短语将被截断，这可能导致模型的错误分类。 将BERT应用于文档分类的另一个明显问题是计算消耗。也就是说，随着输入序列长度的增加，对计算资源的需求急剧增加，因为微调每个变压器层的时间复杂度相对于输入长度呈指数增长。 本文提出了带自适应微调策略的分层BERT模型 hierarchical BERT model with an adaptive fine-tuning strategy来解决上述问题。HAdaBERT模型由两个主要部分组成，以分层方式对文档表示进行建模，包括局部和全局编码器。考虑到文档具有自然的层次结构，即一个文档包含多个句子，每个句子包含多个单词，我们使用局部编码器学习句子级特征，而全局编码器将这些特征组合为最终表示。与现有的plm直接截断文档到最大输入长度相比，提议的HAdaBERT使用文档的一部分作为区域，而不是使用一个句子作为一个区域。我们引入了一个容器划分策略来获得有效的本地信息，从而保留语法依赖关系。这样，容器中的关键信息和会被提取到局部编码器。全局编码器使用基于注意力的门控记忆网络学习顺序组合容器之间的语法关系。通过在分层体系结构中使用这两种编码器，该模型可以有效地捕获长文档中的本地信息和长期依赖关系。此外，提出了一种针对每个输入样本的自适应微调策略，该策略网络可以自适应地选择最佳的BERT微调层，以提高模型在训练和推理过程中的性能。这种策略是一种自动的通用方法，可以扩展到其他预训练的语言模型。 在多个语料库上进行了实证实验，包括AAPD、Reuters、IMDB和Yelp-2013数据集。对比结果表明，所提出的HAdaBERT模型在文档分类方面优于现有的几种神经网络。此外，该模型可以有效地解决以往文档分类方法的局限性。它也与现有的学习短句表示的模型竞争。另一个观察结果是，自适应微调策略通过动态选择微调层来实现最佳性能改进。 本文的其余部分组织如下。第二节介绍和回顾了文献分类的相关工作。第3节描述了提出的分层BERT模型和自适应微调策略。对比实验在第4节中进行。最后，在第5部分得出结论 相关工作文档级分类是自然语言处理中的一项基本任务，也是一项具有挑战性的任务。本节简要回顾现有的文档级分类方法，包括传统的、分层的和预训练的神经网络。 Conventional neural networks传统神经网络 通常用于文档分类任务的深度神经网络模型有cnn[23-25]和rnn[9,26]。这些模型将文档表示为具有语义和语法信息的分布式表示。 rnn中的梯度爆炸&#x2F;消失问题可能导致模型无法捕获长期依赖关系。为了解决这个问题，LSTM[29]和GRU[10]网络使用允许显式内存更新和传递的内存单元。这两种模型都学习上下文信息，这些信息有助于捕获长文本的语义。此外，Tai等[11]提出了一种根据依赖解析树合并语法信息的树- lstm网络。Zhou等人[30]提出了一种循环CNN (RCNN)模型，该模型扩展了现有的卷积层作为循环架构。Yang等人[4]提出了一种序列到序列模型，用于捕获多标签文档分类中多个标签之间的内在关系。此外，记忆网络[31-33]引入了一种类似于LSTM网络的存储单元的外部存储器，能够存储长文本的信息。 引入了注意机制[12]来选择重要信息，从而提高模型的性能。注意机制能够捕获文本的重要信息，从而更好地学习文本表示，并且可以很好地与各种神经编码器配合使用。考虑到这种方法，Zheng等[37]提出了一种基于自交互关注的文档分类机制。注意机制有两个局限性。首先，它无法感知位置信息，导致无法学习句子内的高级语言特征，如表达的转变或递进表达。此外，当上下文中其他不太重要的单词数量增加时，在很长的文档中强调关键字可能会有困难，因为通过注意机制分配的关键字的权重会被稀释，并且在模型训练期间难以利用。 Hierarchical neural networks层次神经网络 近年来，层次神经网络[15,38]被广泛用于文档分类。为了获取句子间的句法信息，Tang等[38]设计了一种分层架构，使用CNN和LSTM网络结合词嵌入学习文档表示，然后采用GRU网络对文档内的句法信息进行编码。Xu等人[39]提出了一种缓存LSTM模型，该模型应用了一组具有不同遗忘门的LSTM细胞。遗忘率高的门可以学习局部特征，遗忘率低的门可以捕捉全局特征。为了将自注意机制扩展为层次结构，Yang等人[15]提出了一个在句子和文档两个层次上都有注意机制的层次模型。Yin等[42]将任务作为理解问题，提出了一种分层交互的基于注意力的模型来构建文档表示。此外，对于产品评论，一些研究证明，将情感信息与额外的特征(如用户和产品消息)结合起来，对于最终的极性分类是有用的[43-45]。 Pretrained neural networks预训练神经网络 通过使用自关注机制，transformer[19]不能直接使用GRU和LSTM网络的顺序结构，但是允许模型并行训练。为了进一步改进transformer，Gong等[46]提出了多头注意力算法，并将分层结构引入自注意力机制。但是，转换器可能会忽略文本的本地依赖关系和时间关系。随着序列长度的增加，变压器的存储和计算复杂度迅速增加。 已经提出了几种用于文本分类的transformer预训练语言模型(plm)[16,47]。其基本思想是利用大量未标记的数据，通过无监督mask语言模型或下一句预测对语言模型进行预训练学习句法和语义信息，利用这种方法，Adhikari等[20]首先引入了BERT模型[16]，采用截断策略作为DocBERT进行文档分类，取得了良好的性能。Liu等[18]提出了RoBERTa，它使用快速微调的BERT训练程序，成功地使用分段预测进行文档分类，并显示出改进的结果,Lan等[17]提出了ALBERT模型，该模型引入了分解嵌入参数化和跨层参数共享两种参数约简技术，降低了内存消耗，提高了训练速度。 对于文档分类，现有的plm受到输入长度的限制。使用截断策略时，模型会丢失一部分用于分类的信息，当重要的内容落在被截断的部分时，会产生深远的影响，降低文档分类模型的性能。另一个问题是，基于bert的模型的预训练和微调都会产生相当大的计算资源成本。随着输入长度的增加，消耗的资源急剧增加，注意力权重被稀释。 Hierarchical BERT with adaptive fine-tuning具有自适应微调的分层BERT 它由两个主要部分组成，以分层方式对文档表示进行建模，包括本地和全局的编码器，输入文档首先被分成几个容器。在每个容器中，基于bert的模型被微调以提取高质量的局部特征。以连续的局部特征为输入，采用基于注意力的门控记忆网络作为全局编码器，学习容器间的长期依赖关系。提出的HAdaBERT模型可以有效地捕获局部和全局信息用于文档分类。 微调BERT应用在局部编码，adaptive fine-tuning自适应调整BERT。 local encoder对于每个给定的文档，将文档划分为v个容器，表示为D&#x3D; [r1,r2, . . . ,rv]。每个容器又由一系列的token组成，$$r_{i} &#x3D; [x_{i1},x_{i2},…,x_{ic}]$$其中c是容器的容量。在直观的区域划分策略中，将文档中的每个单独的句子作为一个区域。例如，由三个句子组成的文档有三个区域。这个简单的策略是非常不平衡的，因为一个大的句子长度边距，很长和很短的句子都出现在文档中。另一种简单的方法是将文档划分为固定长度的区域。这种策略破坏了文档中的语法关系，可能导致性能下降。 容器策略：几个固定容量c的容器基于容器尽可能多的装句子的想法装载句子。我们依次将句子放入容器中，直到文本长度超过容量c。不够的用0值填充到c。值得注意的是，我们在每个相邻的容器之间建立了一个重叠句子。也就是说，前一个容器的最后一句话和下一个容器的第一句话是相同的。这可以有效地链接这两个容器，使它们不是相互独立的，并且保留了语法依赖性。 为了提取每个容器中的局部特征，我们使用了预训练的语言模型BERT[16]。它在各种NLP任务中取得了令人印象深刻的表现。BERT由多层双向transformer编码器组成[19]，通过无监督学习进行预训练，使用屏蔽语言模型(屏蔽率为15%)或下一句预测。特别地，我们使用了uncase BERT-based1模型，它包含12层transformers，隐藏大小为768。在每个容器中，首先在容器ri的开始和结束处分别添加两个特殊符号[CLS]和[SEP]。然后，我们将每个容器送入BERT模型以获得局部表示ti。$$t_{i}\\in\\mathbb{R}^{dt}$$ $$t_{i}&#x3D;f_{BERT}([x_{i1},x_{i2},…,x_{ic}]:\\theta _{BERT})$$ 其中xi1, xi2，。， xic是第i个容器r中令牌的输入序列。θBERT是BERT模型的可训练参数，为所有容器共享，然后在模型训练时进行微调;dt&#x3D;768是局部表示的维数。 Global encoder为了将所有局部特征顺序编码为文档表示，使用了基于注意力的门控记忆网络来捕获跨容器的远程依赖关系并对全局信息建模。[t1, t2, . . . , tv] as input，首先使用双向LSTM (BiLSTM)网络将局部表示映射到隐藏状态，表示为$$\\underset{h_{i}}{\\rightarrow} &#x3D; LSTM(\\underset{h_{i-1}}{\\rightarrow},t_{i})$$ $$\\underset{h_{i}}{\\leftarrow} &#x3D; LSTM(\\underset{h_{i+1}}{\\leftarrow},t_{i})$$ $$h_{i}&#x3D;[\\underset{h_{i}}{\\rightarrow} ,\\underset{h_{i}}{\\leftarrow}]$$ 为了增强每个局部表示的信息，我们在BiLSTM的输入和输出之间添加残差连接，记为$$k_{i} &#x3D; h_{i}\\oplus t_{i}$$ki表示局部特征，\\oplus 表示相加。 attention-based gated memory network （AGM）层。考虑到并非所有单词和句子对最终分类的贡献都是相同的，我们使用AGM层依次选择重要信息并将其整合到记忆中，以获得远距离关系并保留上下文信息。为了自动选择重要信息，我们使用自注意机制来学习每个局部表示ki的权重，其计算为$$e_{i} &#x3D; tanh(W_{e}k_{i}+b_{i})$$ $$\\alpha &#x3D;\\frac{exp(e_{i})}{\\sum_{j&#x3D;1}^{|D|}exp(e_{j})}$$ 其中，We和be分别表示与注意层相关的权重和偏差，αi是分配给第i个局部表示的权重。AGM层具有循环架构，这有点类似于内存网络。它使用记忆向量m∈R dm来记录局部表示中的重要信息，并使用门g来控制训练过程中需要保留或忘记的相关信息。AGM层的计算如下:$$g_{i}&#x3D;\\sigma (W_{i}k_{i}+U_{g}m_{i-1})$$ $$\\hat{m}&#x3D;tanh(W_{h}k_{i}+g_{i}\\odot U_{h}m_{i-1})$$ $$m_{i}&#x3D;(1-\\alpha_{i}\\odot m_{i-1}+\\alpha_{i}\\odot m_{i})$$权重αi用来控制保留多少过去的信息和吸收多少新的信息。 为了实现，我们应用了两个独立的AGM层对每一步序列的前向和后向信息进行建模。最后的记忆向量是AGM的两个方向的连接，它能够保存过去和未来的信息。我们将前向的最后一个记忆向量和后向的第一个记忆向量连接起来，生成用于最终分类的文档表示o。$$o&#x3D; [\\underset{m_{v}}{\\rightarrow},\\underset{m_{1}}{\\leftarrow}]$$给定训练数据集{Dn, yn}，分类是一个带有softmax函数的单层MLP。损失函数具有分类交叉熵 Adaptive fine-tuning strategy自适应微调策略 对BERT中的所有层进行微调不一定会产生最佳性能。因此，我们使用policy network 选择BERT中需要在训练期间微调或冻结的层。等式1中BERT模型由L层transformer组成。$$f_{BERT}&#x3D;[T1,T2,…,TL]$$该policy network表示如下：$$policy(r)&#x3D;\\left{s_{1},s_{2},…,s_{L}\\right}, s_{i}\\in \\left{0,1 \\right}$$如果si&#x3D;1则对第i层微调，si&#x3D;0则对第i层冻结。我们冻结了BERT中的原始的Ti，创建了一个新的可训练的层$$ \\hat{Ti} $$。第i层的输出计算如下:$$d_{i} &#x3D;s_{i}\\hat{T_{i}(d_{i-1})}+(1-s_{i}T_{i}(d_{i-1})),d_{i}表示第i个BERT层的输出$$policy network由三层transformer组成，容器r的embedding表示作为输入，s的计算如下：$$e_{i} &#x3D; Transformer(r),e_{i}只有两个元素$$ $$s_{i}&#x3D;argmax(e_{i})$$ ei是logit分布，si是离散分布，离散数据导数不适用很难反向传播。于是用的Gumbel-Softmax 。 Gumbel-Softmax是一种从离散数据中采样的方法一种允许定义离散分布的可微的近似抽样的形式的分布。GumbelSoftmax将离散值转换为连续值，从而在端到端反向传播时实现梯度计算和策略网络优化。具体的Gumbel-Softmax实现如下:$$s_{i}&#x3D;softmax(\\frac{log(e_{i}+G_{i})}{\\iota })，\\iota 越小s_{i}就越接近onehot向量$$ $$G_{i}&#x3D;-log(log(u_{i}))$$ Gi是一个随机变量服从独立的均匀分布。ui服从均匀分布U(0,1) 实验数据集 Dev是开发集也称为验证集，从训练集中分出一部分作为开发集，目的是用来选择模型，调整参数评估性能。 几个Baseline模型： 局部编码中容器容量c的影响： 等实验说明了该模型的优越性。 未来未来的工作将探索更有效的局部编码器，并引入强化学习来研究更好的微调策略。","categories":[{"name":"阅读笔记","slug":"阅读笔记","permalink":"https://sandra-feng.github.io/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"学习记录","slug":"学习记录","permalink":"https://sandra-feng.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}]},{"title":"Transformer-based Models for Long Document Classification论文阅读","slug":"Revisiting-Transformer-based-Models-for-Long-Document-Classification论文阅读","date":"2023-07-28T01:33:00.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"Revisiting-Transformer-based-Models-for-Long-Document-Classification论文阅读.html","link":"","permalink":"https://sandra-feng.github.io/Revisiting-Transformer-based-Models-for-Long-Document-Classification%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB.html","excerpt":"","text":"Revisiting Transformer-based Models for Long Document Classification论文阅读摘要比较了不同的基于transformer的长文档分类(TrLDC)方法，这些方法旨在减轻普通转换器编码更长的文本的计算开销，即稀疏注意和分层编码方法。我们在覆盖不同领域的四个文档分类数据集上研究了稀疏注意(例如，局部注意窗口的大小，全局注意的使用)和分层(例如，文档分割策略)转换的几个方面。我们观察到能够处理较长的文本的明显好处，并且根据我们的结果，我们得出了在长文档分类任务上应用基于transformer的模型的实用建议。 引言对于长文档分类来说，截断文本可能会遗漏重要信息，导致分类性能差(图1)。另一个挑战就是每一个token都会关注其他所有token，在多头自注意力的操作中计算开销。使得处理长文本变得非常困难。 对于第二个挑战，长文本的transformer已经出现。但是他们的实验在一个并不好的数据集上。在一些数据集上，BERT的多个变体比基于CNN或rnn的模型表现更差。作者们认为有必要了解基于transformer的模型在对实际较长的文档进行分类时的性能。 贡献：比较了基于transformer架构的不同长文档分类方法:即稀疏关注和分层方法。一些设计选择(如稀疏注意方法中的局部注意窗口大小)可以在不牺牲有效性的情况下提高效率，而一些选择(如分层方法中的文档分割策略)会极大地影响有效性。基于transformer的模型可以在MIMIC-III数据集上优于以前最先进的基于CNN的模型。 问题陈述和数据集我们将文档分类模型分为两个组件:(1)文档编码器，它构建给定文档的向量表示;(2)一个分类器，该分类器预测给定编码向量的单个或多个标签。我们使用基于transformer的编码器来构建文档表示，然后将编码的文档表示作为分类器的输入。我们使用TANH激活的隐藏层，然后是输出层。输出概率通过应用SIGMOID或者SOFTMAX得到。我们主要在MIMIC-III数据集上进行实验 MIMIC-III包含重症监护病房(ICU)出院摘要，每个摘要都使用ICD-9(国际疾病分类，第九次修订版)层次结构用多个标签-诊断和程序进行注释。根据Mullenbach等人(2018)的研究，我们使用前50个频繁标签进行实验 为了解决一般化的问题，我们还使用了来自其他领域的三个数据集:ECtHR来自法律案件，Hyperpartisan 和20 News 均来自新闻文章。 方法Sparse（稀少的）-Attention TransformersBeltagy等人(2020)的Longformer由本地(基于窗口的)注意力和全局注意力组成，这降低了模型的计算复杂性，因此可以部署到处理多达4096个令牌。 BigBird是另一个基于稀疏注意力的Transformer，它使用本地、全局和随机注意力的组合，即所有令牌也会在同一邻域中的令牌之上参加许多随机令牌。这两个模型都是从公共RoBERTa检查点热启动的，并进一步对掩码语言建模进行预训练。据报道，在一系列需要长序列建模的任务中，它们的表现优于RoBERTa。 Hierarchical Transformers文本先被分割成段，每个段应该有少于512个token，每个片段可以使用预训练的transformer编码器，将每个片段第一个token的上下文表示与片段位置嵌入相加作为片段表示。片段编码器transformer分为两部分捕获片段之间的关系和上下文片段表示输出列表。","categories":[{"name":"阅读笔记","slug":"阅读笔记","permalink":"https://sandra-feng.github.io/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"学习记录","slug":"学习记录","permalink":"https://sandra-feng.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}]},{"title":"HAN(Hierarchy Attention Network)模型及实现","slug":"HAN-Hierarchy-Attention-Network-模型","date":"2023-07-23T06:04:21.000Z","updated":"2023-07-23T06:04:21.000Z","comments":true,"path":"HAN-Hierarchy-Attention-Network-模型.html","link":"","permalink":"https://sandra-feng.github.io/HAN-Hierarchy-Attention-Network-%E6%A8%A1%E5%9E%8B.html","excerpt":"","text":"HAN(Hierarchy Attention Network)模型文本由时序性的sentence组成，而sentence则是由时序性的word组成。我们要想根据文章的语义对一篇文章进行分类，模型分两步来进行，首先从单词层面分析每个句子的语义。总结出每个句子的语义后，再将句子综合起来表征整篇文章的语义，并对其进行分类。 模型的结构就如下图所示，分为上下两个 block，两个 block 的结构完全一致，都是由 Bi-GNN 组成特征抽取器，同时添加了注意力机制。下层的 block 做句子级别的特征抽取，得出句子表示，抽取后的句子特征向量作为上层 block 每一时刻的输入，再由上层 block 进行篇章级别的特征抽取得出文本向量，最后还是使用 Softmax 做最后的分类。 实验结果： 模型代码(pytorch)[论文链接](Hierarchical Attention Networks for Document Classification (microsoft.com)) 123456789101112131415161718192021222324252627class Attention(nn.Module): def __init__(self, hidden_size): super(Attention, self).__init__() self._hidden_size = hidden_size #hidden_size作为输入，它表示输入序列的隐藏状态的大小。 # Linear layer for the tanh activation (eq. 5 in paper) # (times two because bidirectional)双向线性层 self._layer1 = nn.Linear(2 * hidden_size, 2 * hidden_size) #两层线性层，layer1和layer2 # Linear layer for the softmax activation (eq. 6 in paper) self._layer2 = nn.Linear(2 * hidden_size, 2 * hidden_size, bias = False) #layer2:该层用于使用softmax激活函数计算注意力权重。它的输入和输出大小为2 * hidden_size。注意权重将决定输入序列中每 #个隐藏状态的关注程度。 def forward(self, hidden_states): &quot;&quot;&quot; 注意力机制的向前传递 param hidden_states:输入序列在时刻T的隐藏状态 return:上下文向量(GRU加权输出)和关注权重 &quot;&quot;&quot; # (see equation 5) u = torch.tanh(self._layer1(hidden_states)) # (see equation 6) alphas = F.softmax(self._layer2(u), dim=1) # --&gt; current dimensions: X x Y x Z # Sentence vectors # (see equation 7) # Apply the attention weights (alphas) to each hidden state sentence = torch.sum(torch.mul(alphas, hidden_states), dim=1) # Return return(sentence, alphas) 12345678910111213141516171819202122232425262728293031class word_encoder(nn.Module): def __init__(self, embedding_size, hidden_size): &quot;&quot;&quot; Word encoder. This part takes a minibatch of input sentences, applies a GRU and attention and returns the sequences. :param embedding_size: Size of the word embedding :param hidden_size: number of hidden units in the word-level GRU &quot;&quot;&quot; super(word_encoder, self).__init__() self._hidden_size = hidden_size self.GRU = nn.GRU(embedding_size, self._hidden_size, bidirectional=True, batch_first=True) self.attention = Attention(self._hidden_size) def forward(self, inputs_embedded, hid_state): &quot;&quot;&quot; :param inputs_embedded: word embeddings of the mini batch at time t (sentence x seq_length) :return: tuple containing: (1) weighted GRU annotations (GRU output weighted by the attention vector) (2) [final hidden state of GRU (unweighted), attention weights] &quot;&quot;&quot; # Bidirectional GRU output_gru, last_hidden_state = self.GRU(inputs_embedded) # Unpack packed sequence output_padded, output_lengths = pad_packed_sequence(output_gru, batch_first=True)#填充序列压紧 # Attention output_attention, att_weights = self.attention(output_padded) # Return return(output_attention.unsqueeze(dim=0), [last_hidden_state, att_weights])#被注意力加权之后的GRU输出 123456789101112131415161718192021222324252627class sentence_encoder(nn.Module): def __init__(self, word_hidden_size, hidden_size): &quot;&quot;&quot; Sentence encoder. This part takes as its input a minibatch of documents which have been created by the word encoder. It applies a GRU, attention and returns the weighted GRU output. :param word_hidden_size: The number of hidden units of the word encoder. :param hidden_size: The number of hidden units used for the sentence encoder. &quot;&quot;&quot; super(sentence_encoder, self).__init__() self._hidden_size = hidden_size # Sentence-GRU self.GRU = nn.GRU(word_hidden_size, self._hidden_size, bidirectional=True, batch_first=True) # Attention self.attention = Attention(hidden_size) def forward(self, encoder_output, hid_state): &quot;&quot;&quot; :param encoder_output: output of the word encoder. :return: weighted annotations created by the sentence GRU &quot;&quot;&quot; # Bidirectional GRU output_gru, last_hidden_state = self.GRU(encoder_output) # Attention output_attention, att_weights = self.attention(output_gru) # Return # (weighted attention vector, hidden states of the sentences) return(output_attention.unsqueeze(dim=0), [last_hidden_state, att_weights]) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687class HAN(nn.Module): def __init__(self, vocab_size: int, embedding_size: int, hidden_size_words: int, hidden_size_sent: int, batch_size: int, num_classes: int, device = &quot;cpu&quot;, dropout_prop = 0): &quot;&quot;&quot; Implementation of a Hierarchical Attention Network (HAN). :param vocab_size: Size of the input vocabulary :param embedding_size: Size of the word embedding :param hidden_size_words: number of hidden units for the word encoder. :param hidden_size_sent: number of hidden units for the sentence encoder. :batch_size: size of the minibatches passed to the HAN. :num_classes: number of output classes in the classification task. &quot;&quot;&quot; super(HAN, self).__init__() self._hidden_size_words = hidden_size_words self._hidden_size_sent = hidden_size_sent self._embedding_dim = (vocab_size, embedding_size) self._num_classes = num_classes self._batch_size = batch_size self._dropout_prop = dropout_prop # Embedding self.embedding = nn.Embedding(vocab_size, embedding_size) # Set up word encoder self._word_encoder = word_encoder(self._embedding_dim[1], self._hidden_size_words) # Set up sentence encoder self._sentence_encoder = sentence_encoder(self._hidden_size_words * 2, self._hidden_size_sent) # Set up a linear layer self._linear1 = nn.Linear(self._hidden_size_sent * 2, self._num_classes) def forward(self, seqs, seq_lens, hid_state_word, hid_state_sent, return_attention_weights = False): &quot;&quot;&quot; :param batch_in: list of input documents of size batch_size input document with dim (sentence x seq_length) :param return_attention_weights: if True, return attention weights :return: tensor of shape (batch_size, num_classes) and, optionally, the attention vectors for the word and sentence encoders. &quot;&quot;&quot; # Placeholders batched_sentences = None batch_length = len(seqs) # If return attention weights if return_attention_weights: word_weights = [] # For each, do ... for i, seqdata in enumerate(zip(seqs,seq_lens)): # Unzip seq, seq_len = seqdata # Embedding embedded = self.embedding(seq) # Pack sequences x_packed = pack_padded_sequence(embedded, seq_len, batch_first=True, enforce_sorted=False) # Word encoder we_out, hid_state = self._word_encoder(x_packed, hid_state_word) # Cat sentences together if batched_sentences is None: batched_sentences = we_out else: batched_sentences = torch.cat((batched_sentences, we_out), 0) # Sentence encoder out_sent, hid_sent = self._sentence_encoder(batched_sentences.permute(1,0,2), hid_state_sent) # Cat the attention weights if return_attention_weights: word_weights.append(hid_state[1].data) # If last sentence if i == batch_length: sentence_weights = hid_sent[1].data # Apply dropout out_sent_dropout = F.dropout(out_sent.squeeze(0), p=self._dropout_prop) # Linear layer &amp; softmax prediction_out = F.softmax(self._linear1(out_sent_dropout), dim = 1) # Return if return_attention_weights: # Compute attention weights for words and sentences return(prediction_out, [word_weights, sentence_weights]) else: return(prediction_out) def init_hidden_sent(self): return Variable(torch.zeros(2, self._batch_size, self._hidden_size_sent)) def init_hidden_word(self): return Variable(torch.zeros(2, self._batch_size, self._hidden_size_words))","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"https://sandra-feng.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"}],"tags":[{"name":"学习记录","slug":"学习记录","permalink":"https://sandra-feng.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}]},{"title":"文本分类综述论文阅读笔记","slug":"文本分类综述阅读笔记","date":"2023-07-23T06:04:21.000Z","updated":"2023-07-23T06:04:21.000Z","comments":true,"path":"文本分类综述阅读笔记.html","link":"","permalink":"https://sandra-feng.github.io/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B0%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.html","excerpt":"","text":"A Survey on Text Classification: From Traditional to Deep Learning论文阅读笔记论文地址： [https://dl.acm.org/doi/pdf/10.1145/3495162]: 中科院三区 文本分类过程流程图。第一个重要步骤是为模型预处理文本数据。 传统方法中的文本表示（特征提取）BOW意味着语料库中的所有单词都形成一个映射数组。根据映射数组，句子可以表示为一个向量。向量中的第i个元素表示句子映射数组中第i个单词的频率。向量是句子的BOW。BOW的核心是用字典大小的向量表示每个文本。向量的单个值表示与其在文本中的固有位置相对应的词频。 N-gram考虑相邻单词的信息，并通过考虑相邻单词来构建字典。它用于计算句子的概率模型。句子的概率表示为句子中每个单词的联合概率。给定第(N−1)个单词的序列，可以通过预测第N个单词的概率来计算句子的概率。为了简化计算，N-gram模型采用马尔可夫假设。一个单词只与它前面的单词有关。因此，N-gram模型执行大小为n的滑动窗口。通过计数和记录所有片段的出现频率，可以使用记录中相关片段的频率计算句子的概率。 TF-IDF使用词频和逆文档频率对文本进行建模。TF是一个词在特定文章中的词频，IDF是包含该词的文章占语料库中文章总数的比例的倒数。TF-IDF是两者的乘积。TF-IDF评估一个词对一组文件或语料库中的一个文档的重要性。一个词的重要性随着它在文档中出现的次数成比例地增加。然而，它在整个语料库中的频率呈反比下降。 word2vec利用本地上下文信息获取词向量，词向量是指一个固定长度的实值向量，指定为语料库中任何词的词向量。word2vec使用两个基本模型:CBOW和Skip-gram。前者是在已知当前单词的上下文的前提下预测当前单词。后者是在已知当前单词时预测上下文。 GloVe同时具有局部上下文和全局统计特征，对词-词共现矩阵中的非零元素进行训练，它使词向量能够包含尽可能多的语义和语法信息。词向量的构建方法是:首先基于语料库构建词的共现矩阵，然后基于共现矩阵和GloVe模型学习词向量。最后，根据选择的特征将表示的文本输入到分类器中。 传统分类器PGM-based方法。概率图模型(Probabilistic Graphical Models, PGMs)表达图中特征之间的条件依赖关系，如贝叶斯网络[25]。它是概率论和图论的结合。Naïve贝叶斯(NB)：以先验计算后验。朴素贝叶斯公式 KNN-based。KNN (k近邻)算法[9]的核心是通过在k个最近的样本上找到样本最多的类别来对未标记的样本进行分类。它是一个简单的分类器，不需要建立模型，并且可以通过快速获得k个最近邻来降低复杂性。 **支持向量机(SVM)**。 决策树(DT)[49]是一种有监督的树结构学习方法，反映了分而治之的思想，并且是递归构建的。该算法学习析取表达式，对含有噪声的文本具有鲁棒性。 Integration-based Methods集成的方法。传统的集成算法有自举聚合(bootstrap aggregation)，如RF[14]，增强(boosting)，如Adaptive boosting (AdaBoost)[56]和XGBoost[15]，以及叠加。 深度学习模型一些任务：情感分析(SA)、话题标注(TL)、新闻分类(NC)、问答(QA)、对话行为分类(DAC)、自然语言推理(NLI)和关系分类(RC) 递归神经网络ReNN MLP多层感知机 循环神经网络RNN 自然语言推理(NLI)。[181]通过测量每对句子之间的语义相似性来预测一个文本的意义是否可以从另一个文本中推断出来。 CNN-based方法。对于文本分类，需要将文本表示为类似于图像表示的向量，并且可以从多个角度过滤文本特征，如图7所示。首先，将输入文本的词向量拼接成一个矩阵。然后将矩阵送入卷积层，该层包含多个不同维数的滤波器。最后，卷积层的结果经过池化层，并将池化结果连接起来，得到文本的最终向量表示。类别由最终向量预测。为了尝试使用CNN进行文本分类任务，Kim引入了一种无偏卷积神经网络模型，称为TextCNN[17] Attention-based Methods。层次注意网络(Hierarchical Attention Network, HAN)，通过利用文本中极具信息量的成分来获得更好的可视化效果，如图8所示。HAN包括两个编码器和两个注意层。注意机制让模型对特定的输入给予不同的注意。它首先将关键词聚合成句子向量，然后将重要句子向量聚合成文本向量。基于关注的模型可以了解每个单词和句子对分类判断的贡献程度，通过两个关注层次，有利于应用和分析。HAPN[124]用于少镜头文本分类。 Self-attention通过在句子中构造K、Q和V矩阵来捕获句子中单词的权重分布，这些矩阵可以捕获对文本分类的长期依赖关系。所有的输出向量都可以并行计算。Lin等[114]在句子表示任务中使用源标记自注意来探索每个标记对整个句子的权重。为了捕获远程依赖关系，双向块自关注网络(Bi-BloSAN)[120]对按顺序分割的每个块使用一个块内自关注网络(SAN)，并对输出使用一个块间SAN。 预训练的语言模型[198]可以有效地学习全局语义表示，并显著提高NLP任务，包括文本分类。它通常采用无监督的方法自动挖掘语义知识，然后构建预训练目标，使机器能够学习理解语义。OpenAI GPT [199], and BERT [19]. ELMo [118]，RoBERTa[140]，XLNet[138] 图神经网络GNN-based Methods，文本T &#x3D; [T1,T2,T3,T4]和文本中的单词X &#x3D; [x1, x2, x3, x4, x5, x6]，定义为节点，构建成图结构。图节点由粗体黑色边连接，黑色边表示文档-单词边和单词-单词边。每个词-词边的权重通常表示它们在语料库中的共现频率。然后，通过隐藏层表示单词和文本。最后，通过图来预测所有输入文本的标签。DGCNN[153]是一种将文本转换为词图的graph-CNN，具有使用CNN模型学习不同层次语义的优势。TextGCN为整个数据集构建异构词文本图，并捕获全局词共现信息。图注意网络(Graph ATtention network, GAT)[212]通过关注其邻居而采用掩膜自注意层。因此，提出了一些基于gat的模型来计算每个节点的隐藏表示。异构图注意网络(Heterogeneous Graph ATtention networks, HGAT)[213]采用双级注意机制，学习当前节点中不同相邻节点和节点类型的重要性。该模型在图上传播信息并捕获关系，以解决半监督短文本分类的语义稀疏性问题。 其他网络：孪生神经网络(twin NN)，虚拟对抗训练(VAT)，强化学习(RL)。 总结： RNN按顺序计算，不能并行计算。RNN的缺点使其在模型趋于更深入、参数更多的当前趋势下难以成为主流。 CNN通过卷积核从文本向量中提取特征。卷积核捕获的特征的数量与其大小有关，从理论上讲，CNN的深度足以捕捉远距离的特征。由于深层网络参数优化方法的不足，以及池化层造成的位置信息的丢失，深层网络并没有带来明显的改善。与RNN相比，CNN具有并行计算能力，改进后的CNN可以有效地保留位置信息。但是，它对远距离的特征捕获能力较弱。 GNN为文本构建图形。当设计出有效的图结构时，学习到的表示可以更好地捕获结构信息。 Transformer将输入文本视为一个完全连接的图，在边缘上具有注意评分权重。该算法具有并行计算能力，能够高效地通过自注意提取不同单词之间的特征，解决短时记忆问题。然而，Transformer中的注意力机制计算量很大，特别是在处理长序列时。最近提出了一些用于Transformer计算复杂度的改进模型[146,225]。总的来说，Transformer是文本分类的更好选择。 数据集这里只取新闻分类的数据集。20Newsgroups (20NG) [34], AG News (AG) [93, 234], R8 [235], R52 [235], Sogou News (Sogou)[136], and so on. 20NG是一个新闻组文本数据集。它有20个类别，每个类别的数量相同，包括18,846个文本。 AG News是一个搜索学术界新闻的搜索引擎，选择了四个最大的课程。它使用每个新闻的标题和描述字段。AG包含120,000个培训文本和7,600个测试文本。 R8和R52是两个子集，它们是Reuters的子集[252]。R8有8个类别，分为2189个测试文件和5485个培训课程。R52有52个类别，分为6532个培训文件和2568个测试文件。 搜狗结合了两个数据集，包括搜狗新闻集和搜狗新闻集。每个文本的标签是URL中的域名。 在评价文本分类模型方面，准确率和F1分数是评价文本分类方法最常用的指标。 在新闻分类任务中BLSTM-2DCNN[77]的精确度最高。将双向LSTM (BiLSTM)与二维最大池化相结合。它使用二维卷积来采样矩阵中更有意义的信息，并通过BiLSTM更好地理解上下文。此外，Xue等[189]提出了结合BiLSTM和CNN层的MTNA来解决方面类别分类和方面术语提取任务。 未来研究挑战对于文本分类任务，无论是传统方法还是深度学习方法，数据都是模型性能的关键。本文研究的文本数据主要包括多章节、短文本、跨语言、多标签、少样本文本等。针对这些数据的特点，目前存在的技术挑战如下: 外部知识。众所周知，深度神经网络中输入的有益信息越多，其性能就越好。例如，一个包含常识性知识的问答系统可以回答关于现实世界的问题，并帮助解决信息不完整的问题。因此，增加外部知识(知识库或知识图)[293,294]是提高模型性能的有效途径。现有知识包括概念信息[100,127,204]、常识知识[223]、知识库信息[295,296]、通用知识图[170]等，增强了文本的语义表示。然而，由于输入规模的限制，对于不同的任务，如何以及添加什么仍然是一个挑战。 现有的模型大多是监督模型，过度依赖于大量的标记数据。当样本量过小或出现零样本时，模型的性能将受到显著影响。新的数据集注释需要花费大量的时间。因此，无监督学习和半监督学习具有很大的潜力。此外，特定领域的文本[297,298]，如金融和医学文本，包含许多特定单词或领域专家可理解的俚语，缩写等，这使得现有的预训练单词向量难以处理。 文本表示。在文本预处理阶段，基于向量空间模型的文本表示方法简单有效。但是，该方法会丢失文本的语义信息，因此限制了基于该方法的应用性能。本文提出的基于语义的文本表示方法耗时太长。因此，高效的基于语义的文本表示方法还需要进一步研究。在基于深度学习的文本分类的文本表示中，词嵌入是主要概念，不同语言对表示单元的描述不同。然后，通过模型学习映射规则，将单词表示为向量的形式。因此，如何设计自适应的数据表示方法更有利于深度学习与具体分类任务的结合。 模型集成。大多数传统和深度学习模型的结构都被用于文本分类，包括集成方法。RNN需要逐步递归得到全局信息。CNN可以获取局部信息，通过多层叠加可以增加传感场，捕获更全面的上下文信息。注意机制学习句子中单词之间的整体依赖关系。转换器模型依赖于注意机制来建立输入和输出之间的全局依赖关系的深度。因此，设计一个集成模型是值得尝试利用这些模型的。 模型的效率。基于深度学习的文本分类模型非常有效，如cnn、rnn、gnn等。但是，存在许多技术上的限制，如网络层深度、正则化问题、网络学习率等。因此，优化算法，提高模型训练速度还有更广阔的发展空间。","categories":[{"name":"阅读笔记","slug":"阅读笔记","permalink":"https://sandra-feng.github.io/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"学习记录","slug":"学习记录","permalink":"https://sandra-feng.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}]},{"title":"从零开始建个小站 - 实操：内容增/删/改","slug":"guide-how-to-build-site-11","date":"2022-05-31T12:20:20.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"guide-how-to-build-site-11.html","link":"","permalink":"https://sandra-feng.github.io/guide-how-to-build-site-11.html","excerpt":"","text":"增：新增文章、页面、图片等 删：删除文章、页面、图片等 改：对已有的文章、页面等进行修改 所有的增删改都需要提交到线上仓库才能看到改变。使用本站提供的项目仓库，提交源代码后，会自动触发渲染发布，然后静态上端网络缓存更新后才能看到最新结果。 注意格式不管是Hugo还是Hexo，他们都只是一种渲染框架，所以MarkDown源代码都需要特定的 Front-matter 标记，也就是两行 --- 中间的那段。 1234567891011121314---title: &#x27;网页模板 pug 基本语法&#x27;categories: - 学编程tags: - 博客建站date: 2021-12-10 15:22:57updated: 2021-12-10 15:22:51toc: truecomments: truekeywords: &#x27;&#x27;description: &#x27;pug原名jade，因版权问题更名为pug，即哈巴狗。与hexo默认模块ejs一样，pug也是一个模板引擎，可用于快速的网站开发，当然也可以用于静态博客网站的设计。本站点现时所用主题manupassant也使用了pug。&#x27;top:--- 以上 Front-matter 是 Hexo 程序的，其中设置项也需要对应的主题支持，如果不是 Hexo 基础 Front-matter ，具体需要添加什么根据主题文档来。 Front-matter 基础配置项可见： Hugo-Front-Matter Hexo-Front-matter","categories":[{"name":"做网站","slug":"做网站","permalink":"https://sandra-feng.github.io/categories/%E5%81%9A%E7%BD%91%E7%AB%99/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://sandra-feng.github.io/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"从零开始建个小站 - 实操：个性设置","slug":"guide-how-to-build-site-10","date":"2022-05-30T12:20:20.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"guide-how-to-build-site-10.html","link":"","permalink":"https://sandra-feng.github.io/guide-how-to-build-site-10.html","excerpt":"","text":"项目仓库克隆下来，网站的各项设置都是默认的，一些标题，作者之类的需要根据自己的实际情况进行修改，个性设置主要是网站根目录的框架配置和主题配置。好在 hugo 和 hexo 配置结构大同小异，而且都支持将配置文件放在网站根目录下，只需要修改配置，今后主题更新只需要同步配置其他也互不影响。 每套用一个主题，渲染出来的网站界面和功能都有所不同，所以除了基础配置，各个主题设计的配置项都可能不一样，所以一定要依照主题模板文档去配置！！！ 一定要依照主题模板文档去配置！！！ 一定要依照主题模板文档去配置！！！ HugoHugo只有一个配置文件，默认的配置功能非常少，只包含网站标题，主页地址和语言，其他功能都依赖主题模板实现。 主题到 官网主题页 去选自己喜欢的，然后依照主题文档去操作，这就是前面要强调三遍的依照主题文档去配置！ 以 Ananke 主题为例： 在 官网主题页 点击 Ananke 主题预览图或标题后，会被导航到主题说明页：https://themes.gohugo.io/themes/gohugo-theme-ananke/ 该页面有主题预览及功能说明等，当然也包括该主题怎么安装，怎么配置，以下也就是搬运主题文档演示一遍 按文档说明安装该主题: 12git submodule add https://github.com/theNewDynamic/gohugo-theme-ananke.git themes/ananke# 作为子模块安装到themes/ananke，后期项目仓库部署时会从主题仓库更新 配置 config.toml，以下仅是演示，详细规则可参阅 Hugo官方配置说明 在主题目录下有个 exampleSite 文件夹，相当于就是一个完整的站点演示了，直接把该目录下的 config.toml 复制替换到项目仓库根目录下，或者复制里面的内容移到已有的 config.toml 里也是一样的，最后配置出来是这样的： 123456789101112131415161718192021222324252627282930313233343536373839404142title = &quot;网站标题&quot;baseURL = &quot;https://yiwangmeng.cn&quot;uglyurls = true # true or false, &#x27;Pretty URLs&#x27; VS &#x27;Ugly URLs&#x27;: https://gohugo.io/content-management/urls/#pretty-urlslanguageCode = &quot;zh-cn&quot; # &quot;en-us&quot; defaulttheme = &quot;ananke&quot; # 指定主题resourceDir = &quot;../resources&quot;DefaultContentLanguage = &quot;zh&quot; # 默认展示语言，和[languages]配置对应SectionPagesMenu = &quot;main&quot;Paginate = 9 # this is set low for demonstrating with dummy content. Set to a higher numbergoogleAnalytics = &quot;enableRobotsTXT = true[languages] [languages.zh] title = &quot;Ananke&quot; weight = 1 contentDir = &quot;content/zh&quot; # 该语言内容存贮目录 # languageDirection = &#x27;rtl&#x27; for Right-To-Left languages [languages.en] title = &quot;Ananke English&quot; weight = 2 contentDir = &quot;content/en&quot;[sitemap] changefreq = &quot;monthly&quot; priority = 0.5 filename = &quot;sitemap.xml&quot;[params] text_color = &quot;&quot; author = &quot;&quot; favicon = &quot;&quot; site_logo = &quot;&quot; description = &quot;The last theme you&#x27;ll ever need. Maybe.&quot; # choose a background color from any on this page: http://tachyons.io/docs/themes/skins/ and preface it with &quot;bg-&quot; background_color_class = &quot;bg-purple&quot; recent_posts_number = 3[[params.ananke_socials]]name = &quot;twitter&quot;url = &quot;https://twitter.com/GoHugoIO&quot; 填充内容 直接把主题 exampleSite 的 static 和 content 拷贝到项目网站根目录，然后启动本地环境预览下： hugo server ，除了自己修改过的个性化信息，其他跟主题演示没区别，然后依葫芦画瓢修改或者新增自己要的内容就可以了，关键还是要依照主题文档操作！ Hexo和Hugo一样，Hexo内容个性化也是靠配置文件完成，展示个性化根据主题而定。Hexo主题可以到 官网主题页 去挑选： 配置文件包含根目录下的 _config.yml 和主题目录下的 _config.yml ，在新版中这俩配置可以合而为一，也可以将主题目录下的 _config.yml 移到根目录下命名为： _config.主题名称.yml ，如 _config.maupassant.yml 表示主题 maupassant 的配置，详见 Hexo主题配置说明。 配置优先级为：项目网站根目录下的 _config.yml 》 _config.maupassant.yml 》主题目录下的 _config.yml ，建议使用 _config.主题名称.yml 方式存储主题配置。 Hexo和Hugo一样，基础配置较少，剩下的请参照主题文档配置！请参照主题文档配置！请参照主题文档配置！","categories":[{"name":"做网站","slug":"做网站","permalink":"https://sandra-feng.github.io/categories/%E5%81%9A%E7%BD%91%E7%AB%99/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://sandra-feng.github.io/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"从零开始建个小站 - 实操：本地测试预览","slug":"guide-how-to-build-site-9","date":"2022-05-29T12:20:20.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"guide-how-to-build-site-9.html","link":"","permalink":"https://sandra-feng.github.io/guide-how-to-build-site-9.html","excerpt":"","text":"如果你想在本地就直接看到渲染后的效果，那么还需要安装个本地环境。 HugoHugo 是个golang开发的跨平台程序，无需外部依赖，直接将单程序安装部署在本地就行，此处以Windows操作系统为示例，其他操作系统请见 [Hugo官方安装说明][hogo-install]。 打开 Hugo版本发布页，下载 Windows 版本，建议下载 hugo_extended 版，将程序 hugo.exe 解压到某目录，如 C:\\HUGO\\ ，然后将此目录添加到系统变量中就可以在任何位置直接执行 hugo 命令，可直接打开命令终端使用 set 一键完成： 12# 将 C:\\HUGO\\ 添加到系统 path 变量，请替换成自己的实际路径set path=%path%;C:\\HUGO\\ 如果习惯界面设置，可以百度搜索：Windows 添加path变量 ，设置完成后任意路径下执行命令可见效果： 12$ hugo versionhugo v0.99.1-d524067382e60ce2a2248c3133a1b3af206b6ef1+extended windows/amd64 BuildDate=2022-05-18T11:18:14Z VendorInfo=gohugoio 切换到刚克隆下来的项目仓库「vscode打开文件夹后启动的终端会自动切到当前目录」，预览下网站效果： 123456789101112131415161718192021222324xxx@CVE MINGW64 /d/git/action-hugo (main) //当前所在路径，Git分支$ hugo server //运行本地服务端Start building sites … hugo v0.99.1-d524067382e60ce2a2248c3133a1b3af206b6ef1+extended windows/amd64 BuildDate=2022-05-18T11:18:14Z VendorInfo=gohugoio | ZH | EN -------------------+----+----- Pages | 20 | 19 Paginator pages | 0 | 0 Non-page files | 0 | 0 Static files | 6 | 6 Processed images | 0 | 0 Aliases | 2 | 1 Sitemaps | 2 | 1 Cleaned | 0 | 0Built in 352 msWatching for changes in D:\\git\\action-hugo\\&#123;archetypes,content,data,layouts,static,themes&#125;Watching for config changes in D:\\git\\action-hugo\\config.toml, D:\\git\\action-hugo\\themes\\ananke\\config.yamlEnvironment: &quot;development&quot;Serving pages from memoryRunning in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRenderWeb Server is available at http://localhost:1313/ (bind address 127.0.0.1)Press Ctrl+C to stop 点击返回提示的url调用浏览器打开即能看到效果。 HexoHexo 需要依赖 npm ，所以需要安装 nodejs，直接到 官网 下载安装包一路默认安装，macOS及Linux按官网提示安装即可。安装完成 npm version 有相应提示。 因为要在本地运行查看效果，那么还需要安装 hexo-cli 和 node_modules 依赖： 123xxx@CVE MINGW64 /d/git/action-hexo (main) //当前所在路径，Git分支$ npm install -g hexo-cli //全局安装hexo客户端$ npm install //在hexo仓库根目录下执行，会自动安装预设的模块 安装完成后，执行 hexo version 可以查看效果： 12345xxx@CVE MINGW64 /d/git/action-hexo (main) //当前所在路径，Git分支$ hexo sINFO Validating configINFO Start processingINFO Hexo is running at http://localhost:4000/ . Press Ctrl+C to stop. 点击返回提示的url调用浏览器打开即能看到效果。","categories":[{"name":"做网站","slug":"做网站","permalink":"https://sandra-feng.github.io/categories/%E5%81%9A%E7%BD%91%E7%AB%99/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://sandra-feng.github.io/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"从零开始建个小站 - 实操：代码拉到本地","slug":"guide-how-to-build-site-8","date":"2022-05-28T12:20:20.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"guide-how-to-build-site-8.html","link":"","permalink":"https://sandra-feng.github.io/guide-how-to-build-site-8.html","excerpt":"","text":"工欲善其事必先利其器，Hexo&#x2F;Hugo虽然没后台，选用个好用的编辑器后甚至比WordPress之类的后台还方便。 文档编辑器优秀的MarkDown编辑器不少，Typora、Atom、vscode等都是其中的佼佼者，推荐 vscode： 微软主导开发，全平台开源免费 用户众多，各种功能插件一应俱全 支持目录树管理，所有文件操作都可以在一个界面内完成 支持分屏及实时预览 与Git终端集成，版本管理一目了然，支持 pull 与 push 等界面化操作 作为 MarkDown 编辑器，推荐安装以下扩展 Git History GitLens supercharges Markdown All in One Markdown Preview Mermaid Support Markdown Table Markdown Shortcuts 其他更多扩展根据自己需求去发觉安装，代码美化，自动填充，自动关闭标签等功能应有尽有。 克隆仓库到本地虽然项目仓库主页直接增删改文件都可以，但网页上只能一个一个文件操作，建议还是同步到本地使用，借助编辑器事半功倍，也相当于多了个源码本地备份。 前面已经准备好了 vscode，那么直接在 vscode 中操作。 启动 vscode ，通过快捷 CTRL+~ 或者菜单 Terminal》New Terminal（新建终端） 在打开的终端中，通过 git clone 命令将项目仓库克隆到本地「也可以安装 GitHub Explorer 像下载工具一样界面化操作」 123456# 查看当前所处的目录，vscode打开文件夹后终端默认切到目录路径pwd# 为方便管理，切换到自己需要的目录，此示例是在D盘建了个git目录cd d:\\git# 将仓库包括子项目保存到d:\\git\\REPOSITORYgit clone --recurse-submodules https://github.com/USERNAME/REPOSITORY.git 请将仓库地址换成实际地址，获取方法：打开仓库主页》在文件列表右上方有个 Code ，点击下拉复制，如下图所示： 克隆完成后，通过快捷方式 Ctrl+K Ctrl+O 或者菜单 File（文件）》Open Folder（打开文件夹） 打开刚克隆完的仓库目录。 至此，我们就可以在 vscode 中便捷地增删改网站源文件了。","categories":[{"name":"做网站","slug":"做网站","permalink":"https://sandra-feng.github.io/categories/%E5%81%9A%E7%BD%91%E7%AB%99/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://sandra-feng.github.io/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"从零开始建个小站 - 实操：打通发布流程","slug":"guide-how-to-build-site-7","date":"2022-05-27T12:20:20.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"guide-how-to-build-site-7.html","link":"","permalink":"https://sandra-feng.github.io/guide-how-to-build-site-7.html","excerpt":"","text":"截止当前，我们已经准备好了建站需要的存储仓库，接着还需要配置相应的权限才能顺利对外发布。 配置发布令牌还记得 GitHub 设置中配置的 Personal access token 么，前面叮嘱复制保存下来，下面该派上用场了： 导航到刚创建的项目仓库主页面，在仓库名称下，单击 Settings（设置） 点击左侧的 Secrets》Actions，新建仓库机密 在 Name（名称）输入框中键入机密的名称必须为：ACTION_ACCESS_TOKEN ，Value 框粘贴之前复制保存的那串值 最后单击 Add secret（添加密码） 保存完成 小试牛刀经过之前的一番操作，服务器上一切都准备就绪了，可以试试好使与否，直接在项目仓库中触发个提交就行，比如点击 config.toml 或 config.yml 编辑里面的url，然后保存提交。 项目仓库提交变更后，要不了一分钟会自动将源文件渲染并发布到pages仓库，到pages仓库中可见刚从项目仓库提交过来的分支，要对外访问则要设置为pages指定分支。 设置pages分支前文已经提到pages分支了，用户pages仓库为：&lt;owner&gt;.github.io，其他则为项目仓库，设置方法都是一样的： 导航到pages仓库主页，点击 Settings（设置），点击左侧 Pages 标签，选择对应的分支 点击保存后，大约5分钟，就可以通过页面上提示的地址对外访问了。","categories":[{"name":"做网站","slug":"做网站","permalink":"https://sandra-feng.github.io/categories/%E5%81%9A%E7%BD%91%E7%AB%99/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://sandra-feng.github.io/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"从零开始建个小站 - 实操：准备存储仓库","slug":"guide-how-to-build-site-6","date":"2022-05-26T12:20:20.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"guide-how-to-build-site-6.html","link":"","permalink":"https://sandra-feng.github.io/guide-how-to-build-site-6.html","excerpt":"","text":"本实操仅针对 建站方案选择 中提及的免费方案：hugo&#x2F;hexo + GitHub + GitHub免费二级域名&#x2F;自备域名，另外的付费方案自带网站后台，界面化的一体操作没什么好演示的，如有需要可求助我们的战略合作伙伴Google和百度。 建立存储仓库虽然可以通过分支控制源文件和对外访问文件，但还是建议分仓库存储： 公开：GitHubPages仓库：页面文件对外访问 私有：项目仓库：存储网站源文件，只有自己可见防止提交记录、网站配置等机密信息外泄 公开：GitHubPages仓库 登录GitHub，在任何页面的右上角，使用 + 下拉菜单选择 New repository（新建仓库） 因为要对外访问，所以名称必须为：&lt;owner&gt;.github.io，且可见性必须为 public(公开)： 此时只是个空仓库备用，等有提交了再来设置pages 私有：项目仓库项目仓库保存着网站的源代码，控制网站的输出内容和页面样式。这里用导入模板仓库的方法，快速生成具有与模板仓库相同的目录结构和文件的新仓库，该模板仓库实现功能： 自动将源文件渲染并发布到GitHubPages仓库pages分支 自动判断配置的网站域名，并决定是否需要绑定 CNAME 自动渲染发布到当前项目仓库pages分支，如果不设置为私有仓库可作为项目主页访问 pages分支只保留最后1次提交记录 打开 这个模板仓库 主页面，根据自己的需求选择 action-hugo 或者 action-hexo 等仓库任选其一 在文件列表上方，单击 Use this template（使用此模板） 输入你想要的仓库名称和描述（可选），网站源码仓库可见性建议选择 Private（私有） （可选）要包括模板中所有分支的目录结构和文件，而不仅仅是默认分支，请勾选 Include all branches（包括所有分支） 最后点击 Create repository from template（从模板创建仓库）","categories":[{"name":"做网站","slug":"做网站","permalink":"https://sandra-feng.github.io/categories/%E5%81%9A%E7%BD%91%E7%AB%99/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://sandra-feng.github.io/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"从零开始建个小站 - 配置SSH密钥","slug":"guide-how-to-build-site-5","date":"2022-05-25T12:20:20.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"guide-how-to-build-site-5.html","link":"","permalink":"https://sandra-feng.github.io/guide-how-to-build-site-5.html","excerpt":"","text":"相对于前文提到的 GitHub设置 - Personal access token，使用 SSH 密钥是另一种更安全的方式。 生成SSH密钥 注意： 2021年8月14号开始，GitHub弃用账密验证Git操作，改用token或SSH密钥 GitHub 在 2022 年 3 月 15 日通过删除较旧的不安全密钥类型提高了安全性，不再支持 DSA 密钥 (ssh-dss)。 在 2021 年 11 月 2 日之前 valid_after 的 RSA 密钥 (ssh-rsa) 可以继续使用任何签名算法。 在该日期之后生成的 RSA 密钥必须使用 SHA-2 签名算法。 某些较旧的客户端可能需要升级才能使用 SHA-2 签名。 当前就相当于强制用户使用超长随机串密码，安全加强是好事，遵循规则使用 SHA-2 签名规则密钥即可： 123# 生成密钥对，一路回车默认即可# 如已有其他密钥对在用，自己改下生成的文件名以防覆盖ssh-keygen -t ed25519 -C &quot;Your_Email&quot; 如果您使用的是不支持 ed25519 算法的旧系统，请使用： 1ssh-keygen -t rsa -b 4096 -C &quot;Your_Email&quot; 更多密钥相关详细信息可参阅 [GitHub官方文档][new-SSH-key] 如果你是一路回车生成密钥对，那么生成的密钥对会保存在：~/.ssh/ 目录下，~ 表示用户目录，如操作系统登录用户名是 xyz ，那么在Windows下路径则为 C:\\Users\\xyz\\.ssh ,macOS&#x2F;Linux下路径为 /home/xyz/.ssh ，其中： 12~/.ssh/id_ed25519 //私钥，保存在本地~/.ssh/id_ed25519.pub //公钥，配置到异端 到此，本地Git环境已准备妥当，下一步将公钥配置到GitHub中就能使用了。 配置密钥为了使用方便，给GitHub添加一个用户密钥，一个密钥可作用于整个账号的增删改查操作。 将 SSH 公钥内容复制到剪贴板「假设都按前面的默认操作」 Windows 打开 Git Bash ，复制粘贴如下命令 12clip &lt; ~/.ssh/id_ed25519.pub//该命令自动将公钥存到剪贴板，直接用文本编辑器打开公钥再复制也是一样的 macOS/Linux 打开 Terminal（终端），复制粘贴如下命令： 12cat ~/.ssh/id_ed25519.pub// 执行完将打印出来的公钥内容完整复制待用 登录GitHub账号后，在任何页面的右上角，单击右上角个人资料照片，然后单击弹出下拉中的 Settings（设置） 选择左侧 Access》 点击 SSH and GPG keys，点击 New SSH key（新 SSH 密钥） 在 Title（标题）字段中，为新密钥添加描述性标签便于识别用途。 例如，如果您使用在个人Mac上，此密钥名称可能是 Personal MacBook。 将前面复制的公钥串粘贴到 Key（密钥）字段 最后点击 Add SSH key（添加 SSH 密钥） 完成添加","categories":[{"name":"做网站","slug":"做网站","permalink":"https://sandra-feng.github.io/categories/%E5%81%9A%E7%BD%91%E7%AB%99/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://sandra-feng.github.io/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"从零开始建个小站 - GitHub设置","slug":"guide-how-to-build-site-4","date":"2022-05-24T12:20:20.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"guide-how-to-build-site-4.html","link":"","permalink":"https://sandra-feng.github.io/guide-how-to-build-site-4.html","excerpt":"","text":"注册GitHub账号如果已有账号则跳过此步骤，直接登录设置即可。如果还没有账号，请访问 https://github.com ，完成注册和邮箱认证，一般用户选择免费套餐就足够用了： 生成 Personal access token取消了用户密码认证后，PAT「Personal access token」成了 GitHub 官方默认的HTTPS认证方式，比SSH密钥安全性差点，但配置简单点。 登录后，在任何页面的右上角，单击右上角个人资料照片，然后单击 Settings（设置） 在左侧栏中，单击 &lt;&gt;开发者设置》Personal access tokens（个人访问令牌） 单击 Generate new token（生成新令牌），如需密码验证输密码验证即可。 Note 行里随便填，写给自己看的，过期时间建议选择 No Expiration（永不过期） 作为安全防范措施，GitHub 会自动删除一年内未使用的个人访问令牌。 如下勾选相关权限 最后点击 Generate token 生成个人访问令牌。 添加完成后，会显示刚添加的令牌，该令牌明文只会显示一次，所以请 务必复制保存下来备用否则就需要删除重新生成，后面项目仓库和本地访问认证都能用得到该令牌。","categories":[{"name":"做网站","slug":"做网站","permalink":"https://sandra-feng.github.io/categories/%E5%81%9A%E7%BD%91%E7%AB%99/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://sandra-feng.github.io/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"从零开始建个小站 - 本地Git配置","slug":"guide-how-to-build-site-3","date":"2022-05-23T12:20:20.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"guide-how-to-build-site-3.html","link":"","permalink":"https://sandra-feng.github.io/guide-how-to-build-site-3.html","excerpt":"","text":"如果你选择的是自备服务器的付费方案，那么直接在服务器上安装环境部署网站程序即可，本文不做详细演示，下文只针对免费方案进行详细说明。 本地 Git 设置安装Git到 Git官网 下载自己操作系统对应的安装包或者按对应命令安装即可。 Windows安装的时候一路选默认 next 到底就行，最后会在文件夹右键菜单中出现 Git Bash Here 方便使用。 macOSInstall homebrew if you don’t already have it, then: 1brew install git LinuxDebian&#x2F;UbuntuFor the latest stable version for your release of Debian&#x2F;Ubuntu 1apt install git CentOS&#x2F;Fedora 1yum install git 配置Git用户信息如前一步图示，随便在哪个文件夹里：点 右键 菜单》点击 Git Bash Here 》启动 Git Bash 「macOS&#x2F;Linux系统打开 Terminal（终端）」，复制粘贴如下命令： 123# 设置Git用户信息git config --global user.name &quot;Your_Name&quot;git config --global user.email &quot;Your_Email&quot; 注意：请将命令中邮箱及用户名替换为自己实际信息","categories":[{"name":"做网站","slug":"做网站","permalink":"https://sandra-feng.github.io/categories/%E5%81%9A%E7%BD%91%E7%AB%99/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://sandra-feng.github.io/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"从零开始建个小站 - 建站方案选择","slug":"guide-how-to-build-site-2","date":"2022-05-22T12:20:20.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"guide-how-to-build-site-2.html","link":"","permalink":"https://sandra-feng.github.io/guide-how-to-build-site-2.html","excerpt":"","text":"既然这是篇小白零基础建站教程，那么就不会涉及带门槛的方案，只是简单罗列了适合新手的案例，其他同等方案或者更复杂方案等熟悉了可以再自行研究。 方案对比 免费：hugo&#x2F;hexo + GitHub + GitHub免费二级域名&#x2F;自备域名 flowchart LR; 本地维护MarkDown内容 -- hugo/hexo渲染 -->本地效果预览 GitHub私有仓库 -- 绑定自备域名 --> 公开pages服务 本地维护MarkDown内容 GitHub私有仓库 -- 触发action自动渲染 --> 公开pages服务 优点：有免费资源，静态页速度快，网站源文件通过git版本管理安全可靠 缺点：需要点MarkDown语法知识，缺界面化管理后台 付费：WordPress&#x2F;Typecho&#x2F;Zblog + 自备服务器 + 自备域名 flowchart LR; 自备域名 -- DNS解析 --> 自备服务器 --> 网站对外服务 网站界面后台维护内容 --> 自备服务器 -- 大陆区服务器 --> ICP备案 --> 网站对外服务 优点：功能强大几乎能满足所有需求，装好后带后台，纯界面操作 缺点：需要自己购买服务器和域名，度对服务器要求高，响应速度相对慢点 准备条件flowchart TB; 免费方案 --必须--> 注册GitHub账号 & 安装Git客户端 注册GitHub账号 --> 创建仓库 & 配置访问令牌 安装Git客户端 --编辑MarkDown源码--> 发布到GitHub 免费方案 --可选--> 安装本地环境 & 装个趁手的编辑器 & 购买域名 装个趁手的编辑器 免费方案： GitHub账号：要使用免费的资源，不得注册个账号绑定才能找得到么？虽然国内Gitee也有，但绑定自己的域名要收费，而且内容要审核，所以还是直接用GitHub吧。当然，你有自己服务器和域名也可以用来替代。 Git客户端：用来同步管理源代码，改了什么一目了然 MarkDown 编辑器：纯手工敲代码是不可能的，借助编辑器事半功倍，而且还能和Git结合，大大降低难度 域名「可选」：花点小钱占个自己的域名赏心悦目，也好打响自己的品牌，万一哪天走了张伟波的运呢？ flowchart LR; 付费方案 --必须--> 购买域名 & 购买服务器 购买服务器 --大陆区服务器--> ICP备案 购买服务器 --> 安装环境并部署网站 & 后台发布内容 付费方案： 域名：虽然也有免费的，但还是建议花钱买，每年几十元 服务器&#x2F;云主机&#x2F;虚拟主机：需要带数据库，支持PHP及安装扩展 ICP备案：如果用大陆区服务器，必须先工信部ICP备案后才可用，大约需耗时6周","categories":[{"name":"做网站","slug":"做网站","permalink":"https://sandra-feng.github.io/categories/%E5%81%9A%E7%BD%91%E7%AB%99/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://sandra-feng.github.io/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"从零开始建个小站 - 前置知识","slug":"guide-how-to-build-site-1","date":"2022-05-21T12:20:20.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"guide-how-to-build-site-1.html","link":"","permalink":"https://sandra-feng.github.io/guide-how-to-build-site-1.html","excerpt":"","text":"建站须知 虽然说是零基础建站，但一些互联网及计算机基本知识技能还是不能少的，如怎么安装软件，怎么敲命令，怎么解析域名…… 建站就需要文件托管服务，如上传到GitHub仓库，自己买的云服务器，虚拟主机等 网站对外需要有IP或者域名（一般都不会直接IP对外服务），所以要么用GitHub提供免费的二级域名，要么自行购买域名并解析到文件托管服务器 建站时会涉及各种配置设置，而且各程序，各主题都不尽相同，都需要根据实际对象依照文档进行配置，所以需要具备阅读文档的能力 基本概念 名词 解释说明 git 大名鼎鼎的分布式版本管理工具，每个版本改了什么一目了然 GitHub 版本管理托管商，全球最大的男性交友社区 action GitHub提供的在线执行环境，类似于一个虚拟机 pages GitHub提供的网页托管访问服务，每用户一个免费二级域名 npm 依赖包管理工具，各种套娃 MarkDown 轻量标记语言，写文档必备技能 服务器&#x2F;云主机 存放文件24小时在线提供网络访问服务的计算机 域名 互联网上便于人类识别记忆的访问地址 ICP备案 大陆境内服务器需要，有问题方便FBI请喝茶或上门送温暖 主题&#x2F;模板 套用后实展现相应的界面外观及功能 网站程序选型网站程序选型主要依据环境依赖程度和维护难度，以及网络上免费资源可持续性考虑，对大多数普通用户，建议： 首选 hugo/hexo：HTML静态页渲染框架，速度快，可免费托管到GitHub仓库，MarkDown文档维护，主题多可满足大部分需求。大部分用户建议选择 hugo 程序，无需安装麻烦的依赖，而且有将近400套各式主题可选。 其次 WordPress：需自备服务器，没有免费资源可用，但有后台界面，网络上用户多，插件多，文档教程多 市面上网站程序比较多，罗列了几个比较主流的框架，更多可以自行通过搜索引擎查找对应文档。 程序框架 环境依赖 维护难度 推荐度 常见用途 hugo &#x2F; ★ ★★★★★ 个人网站，企业官网，在线文档，求职简历 hexo nodejs ★★ ★★★★☆ 个人网站，企业官网，在线文档，求职简历 gitbook nodejs ★★★ ★★ 在线文档 vuepress nodejs ★★ ★★★ 个人网站，在线文档 docsy nodejs ★★★ ★★★ 在线文档 WordPress MySQL，PHP ★★ ★★★★ 个人网站，企业官网 Typecho MySQL，PHP ★★☆ ★★★☆ 个人网站，企业官网 Zblog MySQL&#x2F;SQLite，PHP ★★ ★★★ 个人网站，企业官网 PS：维护难度和推荐度都是主观意见，推荐度高主要是基于部署简单，可选主题多，互联网免费资源多，对最终实现的功能需求未做考虑，大部分情况根据自己实际需求考量。","categories":[{"name":"做网站","slug":"做网站","permalink":"https://sandra-feng.github.io/categories/%E5%81%9A%E7%BD%91%E7%AB%99/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://sandra-feng.github.io/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"从零开始建个小站 - 其他问题","slug":"guide-how-to-build-site-12","date":"2022-05-21T12:20:20.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"guide-how-to-build-site-12.html","link":"","permalink":"https://sandra-feng.github.io/guide-how-to-build-site-12.html","excerpt":"","text":"如有问题，请留言。","categories":[{"name":"做网站","slug":"做网站","permalink":"https://sandra-feng.github.io/categories/%E5%81%9A%E7%BD%91%E7%AB%99/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://sandra-feng.github.io/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"从零开始建个小站 - 教程导航","slug":"guide-how-to-build-site-0","date":"2022-05-20T12:20:20.000Z","updated":"2024-01-18T12:01:14.142Z","comments":true,"path":"guide-how-to-build-site-0.html","link":"","permalink":"https://sandra-feng.github.io/guide-how-to-build-site-0.html","excerpt":"","text":"引言互联网时代，大家都想在浩瀚的网络世界留下点印记。 虽然有微信朋友圈，QQ空间，微博等可以记录点点滴滴，但他们要么是没法扩大圈子，要么是加以各种限制，到头来这些数据产权还都属于马家，更不用谈某天实现增值获取收益，寄人篱下终究不如自己做主：建个自己掌控的网站！ 本教程旨在试图引导小白从零开始，免费或者低成本建个自己的小站。 教程目录 从零开始建个小站 - 前置知识 从零开始建个小站 - 建站方案选择 从零开始建个小站 - 本地Git配置 从零开始建个小站 - GitHub设置 从零开始建个小站 - 配置SSH密钥 从零开始建个小站 - 实操：准备存储仓库 从零开始建个小站 - 实操：打通发布流程 从零开始建个小站 - 实操：代码拉到本地 从零开始建个小站 - 实操：本地测试预览 从零开始建个小站 - 实操：个性设置 从零开始建个小站 - 实操：内容增&#x2F;删&#x2F;改 从零开始建个小站 - 其他问题","categories":[{"name":"做网站","slug":"做网站","permalink":"https://sandra-feng.github.io/categories/%E5%81%9A%E7%BD%91%E7%AB%99/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://sandra-feng.github.io/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"Markdown高级语法","slug":"markdown-advance-syntax","date":"2022-05-08T09:04:21.000Z","updated":"2022-05-08T09:04:21.000Z","comments":true,"path":"markdown-advance-syntax.html","link":"","permalink":"https://sandra-feng.github.io/markdown-advance-syntax.html","excerpt":"只有少数编辑器支持，或者需要安装相应的扩展渲染，使用前请先预览确认。","text":"只有少数编辑器支持，或者需要安装相应的扩展渲染，使用前请先预览确认。 定义列表1234Term 1Term 2: Definition A: Definition B 会被编译成 123456&lt;dl&gt;&lt;dd&gt;Term 1&lt;/dd&gt;&lt;dd&gt;Term 2&lt;/dd&gt;&lt;dt&gt;Definition A&lt;dt&gt;&lt;dt&gt;Definition A&lt;dt&gt;&lt;/dl&gt; 目录通过[TOC]标记来插入目录。 在编辑器不支持[TOC]标记的情况下可以使用添加id的方法构建目录。 123456## Directory* [1.Content one](#chapter1)* [2.Content two](#chapter2)## &lt;span id=&quot;chapter1&quot;&gt;1.Content one&lt;/span&gt;## &lt;span id=&quot;chapter2&quot;&gt;2.Content two&lt;/span&gt; TeX公式内联的TeX公式使用一个美元符号标记。 1$\\Gamma(n) = (n-1)!\\quad\\forall n\\in\\mathbb N$ 会被编译成内联（行内）公式：$\\Gamma(n) &#x3D; (n-1)!\\quad\\forall n\\in\\mathbb N$ TeX公式块用独占一行的两个美元符号来标记。 1$$\\left \\lbrace \\sum_&#123;i=0&#125;^n i^3 = \\frac&#123;(n^2+n)(n+6)&#125;&#123;9&#125; \\right \\rbrace$$ 会被编译成 $$\\left \\lbrace \\sum_{i&#x3D;0}^n i^3 &#x3D; \\frac{(n^2+n)(n+6)}{9} \\right \\rbrace$$ 如果你的编辑器不支持这个功能，可以手动解决。首先引入mathjax脚本： 1&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&quot;&gt;&lt;/script&gt; 之后，在需要插入公式的地方使用 &lt;script&gt; 标签包裹公式： 12345&lt;script type=&quot;math/tex&quot;&gt;\\Gamma(n) = (n-1)!\\quad\\forall n\\in\\mathbb N&lt;/script&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;\\Gamma(z) = \\int_0^\\infty t^&#123;z-1&#125;e^&#123;-t&#125;dt\\,.&lt;/script&gt; 以上公式展示效果，在实际显示过程中，根据网络加载速度不同会有不同的解析展示速度，TeX的语法参考请见这里。 UML图语法为在代码块开始行后面加入语法声明，如 ```mermaid ，然后可以像这样来画uml时序图： sequenceDiagram Alice->>Bob: Hello Bob,how are you? Note right of Bob: Bob thinks Bob-->>Alice: I am fine thanks! and U? Note left of Alice: SB 其MarkDown代码如下： 1234567\\`\\`\\`mermaidsequenceDiagram Alice-&gt;&gt;Bob: Hello Bob,how are you? Note right of Bob: Bob thinks Bob--&gt;&gt;Alice: I am fine thanks! and U? Note left of Alice: SB\\`\\`\\` #代码块标识会被解析，实际上写代码块时不用添加转义符“\\” 时序图的语法请见 这里 或 这里。 uml流程图： graph LR; A-->B & C-->D; 其实现代码如下： 1234\\`\\`\\`mermaidgraph LR; A--&gt;B &amp; C--&gt;D;\\`\\`\\` 流程图的语法请见 这里， 更复杂点的flowchart可以看 这里 。 参考文档 MarkDown文档中如何画出流程图 https://github.com/wizardforcel/markdown-simple-world http://stevenshi.me/2017/06/26/hexo-insert-formula/ https://blog.csdn.net/u013282174/article/details/80666123 https://www.liuyude.com/How_to_make_your_HEXO_blog_support_handwriting_flowchart.html https://mermaidjs.github.io/","categories":[{"name":"学编程","slug":"学编程","permalink":"https://sandra-feng.github.io/categories/%E5%AD%A6%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"混技能","slug":"混技能","permalink":"https://sandra-feng.github.io/tags/%E6%B7%B7%E6%8A%80%E8%83%BD/"}]},{"title":"网页模板 pug 基本语法","slug":"hexo-pug-notes","date":"2021-12-10T07:22:57.000Z","updated":"2021-12-10T07:22:51.000Z","comments":true,"path":"hexo-pug-notes.html","link":"","permalink":"https://sandra-feng.github.io/hexo-pug-notes.html","excerpt":"","text":"前言pug 原名 jade ，因版权问题更名为 pug ，即哈巴狗。与 hexo 默认模块 ejs 一样，pug 也是一个模板引擎，可用于快速的网站开发，当然也可以用于静态博客网站的设计。本站点现时所用主题 manupassant 也使用了 pug 。 本文针对 Hexo 中使用 pug 的情况为例，说明其基本语法。 安装12345# common installnpm install pug# install for hexo blognpm install hexo-renderer-pug --save 语法pug 不同于 html ，前者不需要标签的开和闭，如 html 的 &lt;p&gt;Demo&lt;/p&gt; ，在 pug 使用 p Demo 即可。 缩进pug 对空格敏感，有点类似 python 对制表符tab敏感。pug 使用空格作为缩进符，当然用 soft tab 也可行。同一级标签需保证左对齐。 123div p Hello, world! p Hello, pug. 渲染结果如下： 1234&lt;div&gt; &lt;p&gt;Hellow, world!&lt;/p&gt; &lt;p&gt;Hello, pug.&lt;/p&gt;&lt;/div&gt; 注释pug 使用 //- 或 // 对代码进行注释，前者注释内容不出现在渲染后的 html 文件中，后者反之。 12//- html中不包含此行// html中会包含此行 属性pug 将标签属性存放于括号 () 内，多个属性之间以 逗号 或 空格 分隔。此外，对于标签的 id 和 class ，pug 使用 # 紧跟标签 id ,使用 . 紧跟标签 class ，可以同时设置多个 class 。 12h1#title Test titleimg#name.class1.class2(src=&quot;/test.png&quot; alt=&quot;test&quot;) 渲染结果如下： 12&lt;h1 id=&quot;title&quot;&gt;Test title&lt;/h1&gt;&lt;img id=&quot;name&quot; class=&quot;class1 class2&quot; src=&quot;/test.png&quot; alt=&quot;test&quot;&gt; 包含为了方便代码复用，pug 提供了 include 包含功能，以下代码会将 _partial 目录下的 head.pug 文件内容包含到当前调用的位置。有点 C&#x2F;C++ 中内联函数的意思。 123doctype htmlhtml(lang=&#x27;en&#x27;) include _partial/head.pug 继承下面是一个简单的 base 模板，通过 block 定义了页面头部 head 和内容 body 。块 block 有点类似 C&#x2F;C++ 的抽象函数，需要在继承者中完成定义，填充具体内容。 123456//- base.pughtml head block title body block content 以下文件使用 extends 继承以上模板，通过 block 覆盖或替换原有块 block 。当然，继承者也可以在原有基础上继续扩展。 123456789//- index.pugextends base.pugblock title title &quot;Test title&quot;block content h1 Hello world! block article 定义变量pug中通过 - var name = value 的形式定义变量 123456- var intData = 100- var boolData = false- var stringData = &#x27;Test&#x27;p.int= intDatap.bool= boolDatap.stringData= stringData 需注意的是，在引用变量时，需要在引用位置加上&#x3D;号，否则会默认将变量名当成普通字符串使用。 如果想要将变量与其它字符串常量或是变量连接在一起，就不能用等号了，而是应该用 #&#123;&#125; ，该符号会对大括号内的变量进行求值和转义，最终得到渲染输出的内容。 1234- var girl = &#x27;Lily&#x27;- var boy = &#x27;Jack&#x27;p #&#123;girl&#125; is so beautiful!p And #&#123;boy&#125; is handsome. 条件结构pug 的条件语句与其它语言类似，均是如下这般： 12345678- var A = &#123;value: &#x27;Test&#x27;&#125;- var B = trueif A.value p= A.valueelse if B p= Belse p nothing 迭代pug 中使用 each 和 while 实现循环迭代，each 可以返回当前所在项的索引值，默认从 0 开始计数。 12345678910//- eachol each item in [&#x27;Sun&#x27;, &#x27;Mon&#x27;, &#x27;Tus&#x27;, &#x27;Wen&#x27;, &#x27;Thu&#x27;, &#x27;Fri&#x27;, &#x27;Sat&#x27;] li= item//- get index of each- var week = [&#x27;Sun&#x27;, &#x27;Mon&#x27;, &#x27;Tus&#x27;, &#x27;Wen&#x27;, &#x27;Thu&#x27;, &#x27;Fri&#x27;, &#x27;Sat&#x27;]ol each item, index in week li= index + &#x27;:&#x27; + item 渲染成 html 后： 123456789101112131415161718&lt;ol&gt; &lt;li&gt;Sun&lt;/li&gt; &lt;li&gt;Mon&lt;/li&gt; &lt;li&gt;Tus&lt;/li&gt; &lt;li&gt;Wen&lt;/li&gt; &lt;li&gt;Thu&lt;/li&gt; &lt;li&gt;Fri&lt;/li&gt; &lt;li&gt;Sat&lt;/li&gt;&lt;/ol&gt;&lt;ol&gt; &lt;li&gt;0:Sun&lt;/li&gt; &lt;li&gt;1:Mon&lt;/li&gt; &lt;li&gt;2:Tus&lt;/li&gt; &lt;li&gt;3:Wen&lt;/li&gt; &lt;li&gt;4:Thu&lt;/li&gt; &lt;li&gt;5:Fri&lt;/li&gt; &lt;li&gt;6:Sat&lt;/li&gt;&lt;/ol&gt; while 调用方式如下： 12345//- while- var day = 1ul while day &lt; 7 li= day++ Minixmixin 名曰混入，类似其它编程语言中的函数，也是为了代码复用，可带参数或不带参数，定义方式如下： 1234mixin menu-item(href, name) li span.dot ● a(href=href)= name 其中，menu-item 为调用时所用名称，可认为是函数名，href 及 name 是参数。同上定义变量所说，a(href=href)= name 中第二个 = 是为了将后面的 name 当作参数来处理，而不是当作字符串 &quot;name&quot; 来处理。 调用 mixin 定义的代码块，需通过 + 号紧跟 mixin 名称及参数: 12+menu-item(&#x27;/Archives&#x27;,&#x27;Archives&#x27;)+menu-item(&#x27;/About&#x27;,&#x27;About&#x27;) mixin 之所以称为混入，是因为其语法不局限于函数调用，在 mixin 可以使用块 block 12345678910mixin print(post) if block block else p= post+print(&quot;no block&quot;)+print(&quot;&quot;) div.box p this is the content of block 对应 html 代码： 12&lt;p&gt;no block&lt;/p&gt;&lt;div class=&quot;box&quot;&gt;&lt;p&gt;this is the content of block&lt;/p&gt;&lt;/div&gt; JavaScript 注意以下 pug 语句中第一行的 . 号。 1234567script(type=&#x27;text/javascript&#x27;). var data = &quot;Test&quot; var enable = true if enable console.log(data) else console.log(&#x27;nothing&#x27;) 对应的 JS 代码如下： 12345678&lt;script type=&#x27;text/javascript&#x27;&gt; var data = &quot;Test&quot; var enable = true if enable console.log(data) else console.log(&#x27;nothing&#x27;)&lt;/script&gt; 对于简单脚本，使用 pug 尚可，复杂的还是单独写到 .js 文件中，然后通过 pug 引用方便一些，引用方式如下： 1234script(type=&#x27;text/javascript&#x27;, src=&#x27;/path/to/js&#x27;)//- with hexo function url_forscript(type=&#x27;text/javascript&#x27;, src=url_for(theme.js) + &#x27;/ready.js&#x27;) hexo 相关在 hexo 主题中使用 pug 时，可以通过使用 hexo 提供的全局变量 config ， theme 来分别调用博客根目录下 _config.yml 文件中的参数以及主题根目录下 _config.yml 文件中的参数。 12345//- blog configp= config.description//- theme configp= theme.title 当然，pug 中可以直接使用 hexo 提供的其它全局变量及辅助函数，使用方法详见 hexo 的文档。 示例12345678910111213//- head.pughead meta(http-equiv=&#x27;content-type&#x27;, content=&#x27;text/html; charset=utf-8&#x27;) meta(content=&#x27;width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0&#x27;, name=&#x27;viewport&#x27;) meta(content=&#x27;yes&#x27;, name=&#x27;apple-mobile-web-app-capable&#x27;) meta(content=&#x27;black-translucent&#x27;, name=&#x27;apple-mobile-web-app-status-bar-style&#x27;) meta(content=&#x27;telephone=no&#x27;, name=&#x27;format-detection&#x27;) meta(name=&#x27;description&#x27;, content=config.description) block title link(rel=&#x27;stylesheet&#x27;, type=&#x27;text/css&#x27;, href=url_for(theme.css) + &#x27;/style.css&#x27; + &#x27;?v=&#x27; + theme.version) link(rel=&#x27;Shortcut Icon&#x27;, type=&#x27;image/x-icon&#x27;, href=url_for(&#x27;favicon.png&#x27;)) script(type=&#x27;text/javascript&#x27;, src=&#x27;//cdn.bootcss.com/jquery/3.3.1/jquery.min.js&#x27;) block more 12345678910111213141516//- base.pugdoctype htmlhtml(lang=&#x27;en&#x27;) include _partial/head.pug block more link(rel=&#x27;stylesheet&#x27;, type=&#x27;text/css&#x27;, href=url_for(theme.plugins) + &#x27;/prettify/doxy.css&#x27;) script(type=&#x27;text/javascript&#x27;, src=url_for(theme.js) + &#x27;/ready.js&#x27; + &#x27;?v=&#x27; + theme.version, async) //- body body: #container.box .h-wrapper include _partial/nav-menu.pug // article content block content include _partial/footer.pug 其中: theme.* 为主题配置文件 _config.yml 中的参数 url_for 为 hexo 提供的用于查找资源路径的函数 总结pug 提供了 包含 ，继承 ，Mixin 等多种方式用于代码复用，语法简洁易懂，除了初学时需花费一些时间学习各种标点符号的含义外，其它倒也没有太大困难。 当然啦，pug 还有许多其它特性，但就我目前使用情况而言，以上这些便已足够。 参考 pugjs.org hexo.io&#x2F;zh-cn&#x2F;docs&#x2F; 原文出处 作者：litreily 链接：https://juejin.cn/post/6844903668383236104 来源：掘金 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[{"name":"学编程","slug":"学编程","permalink":"https://sandra-feng.github.io/categories/%E5%AD%A6%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"博客建站","slug":"博客建站","permalink":"https://sandra-feng.github.io/tags/%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99/"}]}],"categories":[{"name":"学编程","slug":"学编程","permalink":"https://sandra-feng.github.io/categories/%E5%AD%A6%E7%BC%96%E7%A8%8B/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"https://sandra-feng.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"代码修改笔记","slug":"代码修改笔记","permalink":"https://sandra-feng.github.io/categories/%E4%BB%A3%E7%A0%81%E4%BF%AE%E6%94%B9%E7%AC%94%E8%AE%B0/"},{"name":"阅读笔记","slug":"阅读笔记","permalink":"https://sandra-feng.github.io/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"name":"做网站","slug":"做网站","permalink":"https://sandra-feng.github.io/categories/%E5%81%9A%E7%BD%91%E7%AB%99/"}],"tags":[{"name":"混技能","slug":"混技能","permalink":"https://sandra-feng.github.io/tags/%E6%B7%B7%E6%8A%80%E8%83%BD/"},{"name":"学习记录","slug":"学习记录","permalink":"https://sandra-feng.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"},{"name":"教程","slug":"教程","permalink":"https://sandra-feng.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"博客建站","slug":"博客建站","permalink":"https://sandra-feng.github.io/tags/%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99/"}]}